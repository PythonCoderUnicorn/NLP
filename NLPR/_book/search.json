[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLPR",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "2  Testing Rmd",
    "section": "",
    "text": "this is a test of Rmd inside a Quarto book"
  },
  {
    "objectID": "Quanteda-corpus.html",
    "href": "Quanteda-corpus.html",
    "title": "2  Quanteda",
    "section": "",
    "text": "This section is for the Quanteda tutorials, which has 3 components (object types).\nThe 3 object types:"
  },
  {
    "objectID": "Quanteda-corpus.html#libraries",
    "href": "Quanteda-corpus.html#libraries",
    "title": "2  Quanteda",
    "section": "2.1 Libraries",
    "text": "2.1 Libraries\nHere is the list of Quanteda libraries to install and load in order to follow along with any part of the tutorial.\n\n# install.packages(\"quanteda\")\n# install.packages(\"quanteda.textmodels\") \n# devtools::install_github(\"quanteda/quanteda.corpora\")\n# devtools::install_github(\"kbenoit/quanteda.dictionaries\")\n# install.packages(\"readtext\")\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\n# used for reading in text data\nlibrary(readtext)\n\nThe readtext package supports: plain text files (.txt), JSON, csv, .tab, .tsv, .xml, pdf, .doc, .docx, etc and can be used to read a book for the Project Gutenberg for text analysis.\nAfter you have properly installed and loaded the libraries, you can access the the data corpus available without the need of reading in a csv file. Load in a corpus from Quanteda package and save it as a variable.\nCorpus available:\n\ndata_corpus_amicus\ndata_corpus_dailnoconf1991\ndata_corpus_EPcoaldebate\ndata_corpus_immigrationnews\ndata_corpus_inaugural most common used one in tutorial\ndata_corpus_irishbudget2010\ndata_corpus_irishbudgets\ndata_corpus_moviereviews\ndata_corpus_sotu State of the Union speeches\ndata_corpus_udhr\ndata_corpus_ukmanifestos\ndata_corpus_ungd2017\n\n\n# to load a corpus and save it as variable name\ncorpus = corpus(data_char_ukimmig2010)"
  },
  {
    "objectID": "Quanteda-corpus.html#corpus",
    "href": "Quanteda-corpus.html#corpus",
    "title": "2  Quanteda",
    "section": "2.2 Corpus",
    "text": "2.2 Corpus\nLoad the corpus data_corpus_ukmanifestos, which is about British election manifestos on immigration and asylum.\n\nbrit_manifesto = corpus(data_corpus_ukmanifestos,\n                        docvars = data.frame(party = names(data_corpus_ukmanifestos)))\n\n\nbrit_manifesto\n\nCorpus consisting of 101 documents and 1 docvar.\nUK_natl_1945_en_Con :\n\"CONSERVATIVE PARTY: 1945  Mr. Churchill's Declaration of Pol...\"\n\nUK_natl_1945_en_Lab :\n\"Labour Party: 1945  Let Us Face the Future: A Declaration of...\"\n\nUK_natl_1945_en_Lib :\n\"LIBERAL MANIFESTO 1945  20 Point Manifesto of the Liberal Pa...\"\n\nUK_natl_1950_en_Con :\n\"CONSERVATIVE PARTY: 1950  This is the Road: The Conservative...\"\n\nUK_natl_1950_en_Lab :\n\"LABOUR PARTY: 1950  Let Us Win Through Together: A Declarati...\"\n\nUK_natl_1950_en_Lib :\n\"LIBERAL PARTY 1950  No Easy Way: Britain's Problems and the ...\"\n\n[ reached max_ndoc ... 95 more documents ]\n\n\n\nhead( summary(brit_manifesto) )\n\n                 Text Types Tokens Sentences               party\n1 UK_natl_1945_en_Con  1752   6679       269 UK_natl_1945_en_Con\n2 UK_natl_1945_en_Lab  1433   5492       234 UK_natl_1945_en_Lab\n3 UK_natl_1945_en_Lib  1208   3729       157 UK_natl_1945_en_Lib\n4 UK_natl_1950_en_Con  2075   8075       366 UK_natl_1950_en_Con\n5 UK_natl_1950_en_Lab  1541   5392       274 UK_natl_1950_en_Lab\n6 UK_natl_1950_en_Lib  1202   3322       136 UK_natl_1950_en_Lib\n\n\n\n2.2.1 docvars\nQuanteda objects keep information associated with documents, these are ‘document level variables’ and are accessed using docvars() function.\n\ninaug_corpus = corpus(data_corpus_inaugural)\n\nhead( docvars(inaug_corpus))\n\n  Year  President FirstName                 Party\n1 1789 Washington    George                  none\n2 1793 Washington    George                  none\n3 1797      Adams      John            Federalist\n4 1801  Jefferson    Thomas Democratic-Republican\n5 1805  Jefferson    Thomas Democratic-Republican\n6 1809    Madison     James Democratic-Republican\n\n\nTo extract docvars variables use the field argument or use the $ like you normally use on a dataframe.\n\ndocvars( inaug_corpus, field = 'Year')\n\n [1] 1789 1793 1797 1801 1805 1809 1813 1817 1821 1825 1829 1833 1837 1841 1845\n[16] 1849 1853 1857 1861 1865 1869 1873 1877 1881 1885 1889 1893 1897 1901 1905\n[31] 1909 1913 1917 1921 1925 1929 1933 1937 1941 1945 1949 1953 1957 1961 1965\n[46] 1969 1973 1977 1981 1985 1989 1993 1997 2001 2005 2009 2013 2017 2021\n\n\nCreate or update docvars, in this example of creating a column for Century.\n\nfloor_ = floor(docvars(inaug_corpus, field = 'Year') / 100)+1\n\ndocvars(inaug_corpus, field = 'Century') = floor_\n\nhead(docvars(inaug_corpus))\n\n  Year  President FirstName                 Party Century\n1 1789 Washington    George                  none      18\n2 1793 Washington    George                  none      18\n3 1797      Adams      John            Federalist      18\n4 1801  Jefferson    Thomas Democratic-Republican      19\n5 1805  Jefferson    Thomas Democratic-Republican      19\n6 1809    Madison     James Democratic-Republican      19\n\n\n\n\n2.2.2 subset\nThis section is about using the corpus_subset() function on all docvars.\n\n#--- get the Inaugural Speeches\n# inaug_corpus\n\n#--- look at the docvars\n# head( docvars(inaug_corpus))\n\n# now we subset the corpus for speeches after 1990\ninaug_corpus_subset1990s = corpus_subset(inaug_corpus, Year >= 1990)\n\n# check the number of documents \nhead(inaug_corpus_subset1990s, 3)\n\nCorpus consisting of 3 documents and 5 docvars.\n1993-Clinton :\n\"My fellow citizens, today we celebrate the mystery of Americ...\"\n\n1997-Clinton :\n\"My fellow citizens: At this last presidential inauguration o...\"\n\n2001-Bush :\n\"President Clinton, distinguished guests and my fellow citize...\"\n\n\nIf you want only specific US Presidents. Note that there are 2 data objects that appear when you type ‘President’, presidents from the {datasets} package and presidential from {ggplot2} package.\n\n# select specific US Presidents\nselected_presidents = c('Obama','Clinton','Carter')\n\ndemocrat_corpus_subset =  corpus_subset(inaug_corpus,\n                                        President %in% selected_presidents\n                                        )\n\n\ndemocrat_corpus_subset\n\nCorpus consisting of 5 documents and 5 docvars.\n1977-Carter :\n\"For myself and for our Nation, I want to thank my predecesso...\"\n\n1993-Clinton :\n\"My fellow citizens, today we celebrate the mystery of Americ...\"\n\n1997-Clinton :\n\"My fellow citizens: At this last presidential inauguration o...\"\n\n2009-Obama :\n\"My fellow citizens: I stand here today humbled by the task b...\"\n\n2013-Obama :\n\"Vice President Biden, Mr. Chief Justice, Members of the Unit...\"\n\n\n\n\n2.2.3 reshape\nYou can reshape the document paragraphs to sentences, which can be restored even after being modified by functions.\n\nUN_corpus = corpus(data_corpus_ungd2017)\n\n# 196 documents, 7 docvars\nhead(UN_corpus)\n\nCorpus consisting of 6 documents and 7 docvars.\nAfghanistan :\n\"As I stand here before the General Assembly today, I am remi...\"\n\nAngola :\n\"On behalf of the Government of the Republic of Angola, allow...\"\n\nAlbania :\n\"How many times has it happened that humankind has been confr...\"\n\nAndorra :\n\"I would like to begin by congratulating the President, Mr. M...\"\n\nUnited Arab Emirates :\n\"I would like to begin by congratulating the President on his...\"\n\nArgentina :\n\"I am very honoured and very happy to be here today represent...\"\n\n\nNow change the document into sentences\n\nUN_corpus_sentences = corpus_reshape(UN_corpus, to= 'sentences')\n\nhead(UN_corpus_sentences)\n\nCorpus consisting of 6 documents and 7 docvars.\nAfghanistan.1 :\n\"As I stand here before the General Assembly today, I am remi...\"\n\nAfghanistan.2 :\n\"Shaped by the Great Depression and tempered by the carnage o...\"\n\nAfghanistan.3 :\n\"The United Nations, the International Monetary Fund, the Wor...\"\n\nAfghanistan.4 :\n\"There can be little doubt that today the scale, scope and sp...\"\n\nAfghanistan.5 :\n\"But future historians will judge those institutions on how t...\"\n\nAfghanistan.6 :\n\"As global leaders, we seek certainty and familiarity in the ...\"\n\n# there is now 16806 number of documents\n\nTo change back\n\nUN_corpus_doc = corpus_reshape(UN_corpus_sentences, to= 'documents')\n\n\n\n2.2.4 segment\nYou can extract segments of text and tags from documents, the use-case for this is analyzing documents or transcripts separately.\nDocument sections\nThe created mini text document that has 6 lines, which has section that uses “##” as a pattern to organize the text into new lines. Here is what the text file contains.\n\n##INTRO This is the introduction.\n##DOC1 This is the first document.  Second sentence in Doc 1.\n##DOC3    Third document starts here.  End of third document.\n##INTRO                                               Document\n##NUMBER                                      Two starts before\n##NUMBER                                                 Three.\n\n\nlibrary(readr)\n\n# read in a simple text file as shown above\ntxt_file = readr::read_file('./doctext.txt')\n\n# need to convert it to a corpus object\ntxt_file_corpus = corpus(txt_file)\n\n# provide the pattern of ## in order for it to be separated into new lines\ntxt_seg = corpus_segment(txt_file_corpus, pattern = \"##*\")\ntxt_seg\n\nCorpus consisting of 6 documents and 1 docvar.\ntext1.1 :\n\"This is the introduction.\"\n\ntext1.2 :\n\"This is the first document.  Second sentence in Doc 1.\"\n\ntext1.3 :\n\"Third document starts here.  End of third document.\"\n\ntext1.4 :\n\"Document\"\n\ntext1.5 :\n\"Two starts before\"\n\ntext1.6 :\n\"Three.\"\n\n\nBy providing a pattern value for the corpus_segment() you can do regular expressions for removing words and symbols.\n\n# transform text into corpus\ncorp_speeches = corpus(\"Mr. Samwell Tarly: Text Analysis.\n                        Mx. Jones: More text analysis?\n                        Mr. Samwell Tarly: we will do it again, and again.\")\n\n# use corpus segment with regular expression pattern matching\ncorp_speakers = corpus_segment(corp_speeches, \n                                pattern = \"\\\\b[A-Z].+\\\\s[A-Z][a-z]+:\", \n                                valuetype = \"regex\")\n\n# bind the rows together\ncbind(docvars(corp_speakers), text = as.character(corp_speakers))\n\n                   pattern                            text\ntext1.1 Mr. Samwell Tarly:                  Text Analysis.\ntext1.2         Mx. Jones:             More text analysis?\ntext1.3 Mr. Samwell Tarly: we will do it again, and again."
  },
  {
    "objectID": "Quanteda-corpus.html#tokens",
    "href": "Quanteda-corpus.html#tokens",
    "title": "2  Quanteda",
    "section": "2.3 Tokens",
    "text": "2.3 Tokens\nTokens segments texts in a corpus (words or sentences) by word boundaries.\n\n# step 1 - load in corpus\nsotu_corpus = corpus(data_corpus_sotu)\n\n# step 2 - pass in the corpus into tokens function\n# using the tokens() you can remove punctuation and numbers\n\nsotu_corpus_tokens = tokens(sotu_corpus, \n                            remove_punct = TRUE,\n                            remove_symbols = TRUE,\n                            remove_numbers = TRUE,\n                            remove_url = TRUE\n                            )\n\nhead(sotu_corpus_tokens, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"Senate\"         \n [5] \"and\"             \"House\"           \"of\"              \"Representatives\"\n [9] \"I\"               \"embrace\"         \"with\"            \"great\"          \n[ ... and 1,073 more ]\n\nWashington-1790b :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"Senate\"         \n [5] \"and\"             \"House\"           \"of\"              \"Representatives\"\n [9] \"In\"              \"meeting\"         \"you\"             \"again\"          \n[ ... and 1,388 more ]\n\nWashington-1791 :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"Senate\"         \n [5] \"and\"             \"House\"           \"of\"              \"Representatives\"\n [9] \"In\"              \"vain\"            \"may\"             \"we\"             \n[ ... and 2,285 more ]\n\n\n\n2.3.1 kwic\nKeyword-in-contexts (kwic) is the method used to search for keywords in corpus documents.\n\n#-- continue using the tokenized corpus of State of the Union\n# sotu_corpus_tokens\n\n# keyword to search the corpus follows the pattern argument\n# our keyword here will be 'love'\n# save kwic in a variable\n\nkeyword_search =  kwic(sotu_corpus_tokens, pattern = 'love')\n\nhead(keyword_search)\n\nKeyword-in-context with 6 matches.                                                                    \n [Washington-1790b, 1327]        wisdom and animated by the | love |\n  [Washington-1793, 1122]  equity proceeding from a sincere | love |\n  [Washington-1795, 1011] acknowledgment to Heaven and pure | love |\n    [Jefferson-1801, 923]      of happiness educated in the | love |\n     [Jefferson-1807, 37]     earlier period than usual The | love |\n   [Jefferson-1808, 2621] unshaken by difficulties in their | love |\n                                       \n of your country In whatever           \n of peace and a liberality             \n to our country to unite               \n of order habituated to self-government\n of peace so much cherished            \n of liberty obedience to law           \n\n\nYou can search for multiple words by using a vector.\n\nsearch_words = c('love','science')\n\nsearched_words = kwic(sotu_corpus_tokens, pattern = search_words)\n\nhead(searched_words)\n\nKeyword-in-context with 6 matches.                                                                       \n   [Washington-1790, 641]   patronage than the promotion of | science |\n [Washington-1790b, 1327]        wisdom and animated by the |  love   |\n  [Washington-1793, 1122]  equity proceeding from a sincere |  love   |\n  [Washington-1795, 1011] acknowledgment to Heaven and pure |  love   |\n  [Washington-1796, 1910]               of our youth in the | science |\n    [Jefferson-1801, 923]      of happiness educated in the |  love   |\n                                       \n and literature Knowledge is in        \n of your country In whatever           \n of peace and a liberality             \n to our country to unite               \n of government In a republic           \n of order habituated to self-government\n\n\nFor multi-word expressions, use the phrase() function after the pattern argument.\n\nus_searchword = kwic(sotu_corpus_tokens, pattern =  phrase(\"United States\"))\n\nhead(us_searchword)\n\nKeyword-in-context with 6 matches.                                                                              \n    [Washington-1790, 47:48]      to the Constitution of the | United States |\n  [Washington-1790, 397:398] aggressors The interests of the | United States |\n  [Washington-1790, 520:521]     weights and measures of the | United States |\n  [Washington-1790, 956:957]  character and interests of the | United States |\n [Washington-1790b, 520:521]              on the part of the | United States |\n [Washington-1790b, 743:744]     and security enjoyed by the | United States |\n                                   \n of which official information has \n require that our intercourse with \n is an object of great             \n are so obviously so deeply        \n renewed their violences with fresh\n reminds us at the same            \n\n\nTo view all of the keyword search results in a RStudio window.\n\n# view(us_searchword)\n\n\n\n2.3.2 select tokens\nYou can remove tokens that you do not want or interest in, this can be either on its own or in combination with stopwords() function.\nSo far we have tokenized the State of the Union corpus and did some keyword searched, but the corpus still has stopwords inside the text, to remove them we use the function tokens_select() and pass in the stopwords along with the language parameter.\n\n#-- continue using sotu corpus\n\nsotu_corpus_tokens_clean = tokens_select(sotu_corpus_tokens,\n              pattern =  stopwords('en'), # 'en' for English\n              selection = 'remove'\n              )\n\nhead(sotu_corpus_tokens_clean, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n [1] \"Fellow-Citizens\" \"Senate\"          \"House\"           \"Representatives\"\n [5] \"embrace\"         \"great\"           \"satisfaction\"    \"opportunity\"    \n [9] \"now\"             \"presents\"        \"congratulating\"  \"present\"        \n[ ... and 488 more ]\n\nWashington-1790b :\n [1] \"Fellow-Citizens\" \"Senate\"          \"House\"           \"Representatives\"\n [5] \"meeting\"         \"feel\"            \"much\"            \"satisfaction\"   \n [9] \"able\"            \"repeat\"          \"congratulations\" \"favorable\"      \n[ ... and 619 more ]\n\nWashington-1791 :\n [1] \"Fellow-Citizens\" \"Senate\"          \"House\"           \"Representatives\"\n [5] \"vain\"            \"may\"             \"expect\"          \"peace\"          \n [9] \"Indians\"         \"frontiers\"       \"long\"            \"lawless\"        \n[ ... and 1,058 more ]\n\n#---- equivalent function call\n#  tokens_remove( sotu_corpus_tokens, pattern= stopwords('en'), padding= FALSE )\n\nFor very specific words that interest you in token selection, you can pass in a vector of those words.\n\nspecific_words = c('love','scien*','reason')\n\nspecific_tokens_select = tokens_select(sotu_corpus_tokens,\n                                       pattern = specific_words,\n                                       padding = FALSE\n                                       )\n\n# Tokens consisting of 241 documents and 6 docvars.\nhead(specific_tokens_select)\n\nTokens consisting of 6 documents and 6 docvars.\nWashington-1790 :\n[1] \"reason\"  \"science\"\n\nWashington-1790b :\n[1] \"love\"\n\nWashington-1791 :\ncharacter(0)\n\nWashington-1792 :\n[1] \"reason\"\n\nWashington-1793 :\n[1] \"reason\" \"love\"  \n\nWashington-1794 :\n[1] \"reason\" \"reason\"\n\n\nTo see the words that surround the selected token, use the window argument, here the window is 5 words.\n\nwindow_token_select = tokens_select(sotu_corpus_tokens, \n              pattern = specific_words, \n              padding = F, \n              window = 5\n              )\n\nhead(window_token_select, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n [1] \"regard\"    \"to\"        \"economy\"   \"There\"     \"was\"       \"reason\"   \n [7] \"to\"        \"hope\"      \"that\"      \"the\"       \"pacific\"   \"patronage\"\n[ ... and 10 more ]\n\nWashington-1790b :\n [1] \"wisdom\"   \"and\"      \"animated\" \"by\"       \"the\"      \"love\"    \n [7] \"of\"       \"your\"     \"country\"  \"In\"       \"whatever\"\n\nWashington-1791 :\ncharacter(0)\n\n\n\n\n2.3.3 compund tokens\nSimilar to what we already did, there is a tokens_compound() function that uses a vector for tokens to search. This will look just like the kwic() function.\n\n#---- kwic() \nsearched_words2 = c(\"americ*\", \"american people\")\n\namerica_kwic = kwic(sotu_corpus_tokens, pattern = searched_words2)\n\nhead(america_kwic)\n\nKeyword-in-context with 6 matches.                                                                          \n  [Washington-1790b, 67]            by a considerable rise of | American |\n   [Washington-1794, 22]               of Heaven by which the | American |\n [Washington-1794, 1988]     over that precious depository of | American |\n  [Washington-1795, 732]         nations of Europe with their | American |\n  [Washington-1796, 773]         the protection and relief of | American |\n      [Adams-1797, 1583] obligations The numerous captures of | American |\n                                   \n stock abroad as well as           \n people became a nation when       \n happiness the Constitution of the \n dependencies have been involved in\n sea-men agents were appointed one \n vessels by the cruisers of        \n\n\n\n#---- tokens_compound\n\namerica_token_compound = tokens_compound(sotu_corpus_tokens,\n                pattern = phrase(searched_words2) \n                )\n\n# head(america_token_compound)\nhead( kwic(america_token_compound, pattern = searched_words2) )\n\nKeyword-in-context with 6 matches.                                                                               \n  [Washington-1790b, 67]            by a considerable rise of |    American    \n   [Washington-1794, 22]               of Heaven by which the | American_people\n [Washington-1794, 1987]     over that precious depository of |    American    \n  [Washington-1795, 732]         nations of Europe with their |    American    \n  [Washington-1796, 773]         the protection and relief of |    American    \n      [Adams-1797, 1583] obligations The numerous captures of |    American    \n                                     \n | stock abroad as well as           \n | became a nation when we           \n | happiness the Constitution of the \n | dependencies have been involved in\n | sea-men agents were appointed one \n | vessels by the cruisers of        \n\n\n\n\n2.3.4 n-grams\nYou can make n-grams in any length from tokens using the tokens_ngrams() which makes a sequence of tokens.\n\n# -- pass in the sotu corpus tokens object\n#    n-gram will be from 2 to 4\n\nsotu_corpus_ngrams = tokens_ngrams(sotu_corpus_tokens, n= 2:4)\n\n# for simplicity, the 1st document is shown\n# 1st document, 20 of the docvars n-grams\n\nhead(sotu_corpus_ngrams[[1]], 20)\n\n [1] \"Fellow-Citizens_of\" \"of_the\"             \"the_Senate\"        \n [4] \"Senate_and\"         \"and_House\"          \"House_of\"          \n [7] \"of_Representatives\" \"Representatives_I\"  \"I_embrace\"         \n[10] \"embrace_with\"       \"with_great\"         \"great_satisfaction\"\n[13] \"satisfaction_the\"   \"the_opportunity\"    \"opportunity_which\" \n[16] \"which_now\"          \"now_presents\"       \"presents_itself\"   \n[19] \"itself_of\"          \"of_congratulating\" \n\n# head(sotu_corpus_ngrams, 3)\n\nYou can skip n-grams\n\nsotu_corpus_ngrams_skip = tokens_ngrams(sotu_corpus_tokens,\n                                        n= 2,\n                                        skip = 1:2\n                                        )\n\n# notice the document returns [1][4] ...\n\nhead(sotu_corpus_ngrams_skip, 2)\n\nTokens consisting of 2 documents and 6 docvars.\nWashington-1790 :\n [1] \"Fellow-Citizens_the\"    \"Fellow-Citizens_Senate\" \"of_Senate\"             \n [4] \"of_and\"                 \"the_and\"                \"the_House\"             \n [7] \"Senate_House\"           \"Senate_of\"              \"and_of\"                \n[10] \"and_Representatives\"    \"House_Representatives\"  \"House_I\"               \n[ ... and 2,153 more ]\n\nWashington-1790b :\n [1] \"Fellow-Citizens_the\"    \"Fellow-Citizens_Senate\" \"of_Senate\"             \n [4] \"of_and\"                 \"the_and\"                \"the_House\"             \n [7] \"Senate_House\"           \"Senate_of\"              \"and_of\"                \n[10] \"and_Representatives\"    \"House_Representatives\"  \"House_In\"              \n[ ... and 2,783 more ]\n\n\nSelective ngrams\nSelecting ngrams based on a keyword, phrase or a vector of words is done by using the tokens_compound() and tokens_select().\n\n#-- select phrases that have 'not'\n\nphrase_not = c(\"not *\",\"not_*\")\n\nsotu_not_ngrams = tokens_compound(sotu_corpus_tokens,\n                pattern = phrase(phrase_not)\n                )\n\nsotu_not_ngrams_select = tokens_select(sotu_not_ngrams, \n                                       # need to include again\n                                       pattern = phrase(phrase_not)\n                                       )\n\nhead(sotu_not_ngrams_select, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n[1] \"not_but\"     \"not_only\"    \"not_I\"       \"not_forbear\"\n\nWashington-1790b :\n[1] \"not_only\"      \"not_fail\"      \"not_less\"      \"not_be\"       \n[5] \"not_overlook\"  \"not_less\"      \"not_think\"     \"not_desirable\"\n[9] \"not_merely\"   \n\nWashington-1791 :\n[1] \"not_fail\"    \"not_only\"    \"not_forbear\" \"not_fail\""
  }
]