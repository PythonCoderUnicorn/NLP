[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLPR",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "4  Testing Rmd",
    "section": "",
    "text": "https://cran.r-project.org/web/packages/tidytext/tidytext.pdf\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.1      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidytext)\nlibrary(gutenbergr)\n\nlook for a book by title name\n\ngutenberg_metadata %>% \n  filter(title == \"The Iliad\")\n\n# A tibble: 4 × 8\n  gutenberg_id title     author gutenberg_autho…¹ langu…² guten…³ rights has_t…⁴\n         <int> <chr>     <chr>              <int> <chr>   <chr>   <chr>  <lgl>  \n1         2199 The Iliad Homer                705 en      Classi… Publi… TRUE   \n2         3059 The Iliad Homer                705 en      Classi… Publi… TRUE   \n3         6130 The Iliad Homer                705 en      Classi… Publi… TRUE   \n4         6150 The Iliad Homer                705 en      Classi… Publi… TRUE   \n# … with abbreviated variable names ¹​gutenberg_author_id, ²​language,\n#   ³​gutenberg_bookshelf, ⁴​has_text\n\n\nfind works by author\n\ngutenberg_works(author == \"Carroll, Lewis\")\n\n# A tibble: 18 × 8\n   gutenberg_id title              author guten…¹ langu…² guten…³ rights has_t…⁴\n          <int> <chr>              <chr>    <int> <chr>   <chr>   <chr>  <lgl>  \n 1           11 \"Alice's Adventur… Carro…       7 en      Childr… Publi… TRUE   \n 2           12 \"Through the Look… Carro…       7 en      Childr… Publi… TRUE   \n 3           13 \"The Hunting of t… Carro…       7 en      Childr… Publi… TRUE   \n 4          620 \"Sylvie and Bruno\" Carro…       7 en      <NA>    Publi… TRUE   \n 5          651 \"Phantasmagoria a… Carro…       7 en      <NA>    Publi… TRUE   \n 6         4763 \"The Game of Logi… Carro…       7 en      Philos… Publi… TRUE   \n 7        19002 \"Alice's Adventur… Carro…       7 en      Childr… Publi… TRUE   \n 8        28696 \"Symbolic Logic\"   Carro…       7 en      Philos… Publi… TRUE   \n 9        28885 \"Alice's Adventur… Carro…       7 en      Banned… Publi… TRUE   \n10        29042 \"A Tangled Tale\"   Carro…       7 en      Mathem… Publi… TRUE   \n11        29888 \"The Hunting of t… Carro…       7 en      <NA>    Publi… TRUE   \n12        33582 \"Rhyme? And Reaso… Carro…       7 en      <NA>    Publi… TRUE   \n13        35497 \"Three Sunsets an… Carro…       7 en      <NA>    Publi… TRUE   \n14        35535 \"Feeding the Mind\" Carro…       7 en      <NA>    Publi… TRUE   \n15        36308 \"Songs From Alice… Carro…       7 en      <NA>    Publi… TRUE   \n16        38065 \"Eight or Nine Wi… Carro…       7 en      <NA>    Publi… TRUE   \n17        48630 \"Sylvie and Bruno… Carro…       7 en      <NA>    Publi… TRUE   \n18        48795 \"Sylvie and Bruno… Carro…       7 en      <NA>    Publi… TRUE   \n# … with abbreviated variable names ¹​gutenberg_author_id, ²​language,\n#   ³​gutenberg_bookshelf, ⁴​has_text\n\n# or use \n\ngutenberg_works(str_detect(author, \"Carroll\"))\n\n# A tibble: 31 × 8\n   gutenberg_id title              author guten…¹ langu…² guten…³ rights has_t…⁴\n          <int> <chr>              <chr>    <int> <chr>   <chr>   <chr>  <lgl>  \n 1           11 \"Alice's Adventur… Carro…       7 en      Childr… Publi… TRUE   \n 2           12 \"Through the Look… Carro…       7 en      Childr… Publi… TRUE   \n 3           13 \"The Hunting of t… Carro…       7 en      Childr… Publi… TRUE   \n 4          620 \"Sylvie and Bruno\" Carro…       7 en      <NA>    Publi… TRUE   \n 5          651 \"Phantasmagoria a… Carro…       7 en      <NA>    Publi… TRUE   \n 6         4763 \"The Game of Logi… Carro…       7 en      Philos… Publi… TRUE   \n 7         5994 \"Our Nervous Frie… Carro…    1906 en      Psycho… Publi… TRUE   \n 8        16861 \"The Wedge of Gol… Goodw…    6978 en      <NA>    Publi… TRUE   \n 9        19002 \"Alice's Adventur… Carro…       7 en      Childr… Publi… TRUE   \n10        23160 \"Solomon's Orbit\"  Carro…   25719 en      Scienc… Publi… TRUE   \n# … with 21 more rows, and abbreviated variable names ¹​gutenberg_author_id,\n#   ²​language, ³​gutenberg_bookshelf, ⁴​has_text\n\n\ndownload a book by id\n\nbook_link = \"https://www.gutenberg.org/cache/epub/174/pg174.txt\"\n\nbook_file = read_file( book_link )\n\nbook_df = tibble( book_file )\n\nbook_tokens = book_df %>% \n  unnest_tokens(input = book_file, output = word)\n\nbook_clean = book_tokens %>% \n  anti_join(stop_words, by='word')\n\n\nbook_clean %>% \n  filter(word != c(\"gutenberg\",\"project\")) %>% \n  count(word, sort = T)\n\nWarning in word != c(\"gutenberg\", \"project\"): longer object length is not a\nmultiple of shorter object length\n\n\n# A tibble: 6,748 × 2\n   word       n\n   <chr>  <int>\n 1 dorian   414\n 2 don’t    255\n 3 lord     248\n 4 life     230\n 5 henry    223\n 6 gray     193\n 7 harry    174\n 8 basil    153\n 9 love     111\n10 time     110\n# … with 6,738 more rows\n\n\nfind the words in context\n\nlibrary(quanteda)\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"packedMatrix\" of class \"mMatrix\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"packedMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.2.3\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 4 of 4 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nbook_tokens_q = tokens(book_file)\n\nkwic_word = \"love\"\n\nkwic(book_tokens_q, pattern= kwic_word )\n\nKeyword-in-context with 109 matches.                                                     \n  [text1, 1973]            . I have grown to | love |\n  [text1, 5878]     only the trivial side of | love |\n [text1, 12300]                  it? I am in | love |\n [text1, 13117]               \" Why, even in | love |\n [text1, 16078]   fascinating in this son of | love |\n [text1, 17265] quite invaluable. They would | love |\n [text1, 19362]         . She was usually in | love |\n [text1, 19818]             I am charmed, my | love |\n [text1, 20103]             I am too much in | love |\n [text1, 20133]             \" Who are you in | love |\n [text1, 21014]        you will always be in | love |\n [text1, 21016]       always be in love with | love |\n [text1, 21106]          boy, the people who | love |\n [text1, 21879]           . Why should I not | love |\n [text1, 21886]                ? Harry, I do | love |\n [text1, 22401]             . When one is in | love |\n [text1, 23519]            she has genius. I | love |\n [text1, 23527]          and I must make her | love |\n [text1, 23548]       to charm Sibyl Vane to | love |\n [text1, 25342]      results. His sudden mad | love |\n [text1, 25784]      what does money matter? | Love |\n [text1, 25965]                her dress.\" I | love |\n [text1, 26236]               ,\" why does he | love |\n [text1, 26245]               ? I know why I | love |\n [text1, 26249]                I love him. I | love |\n [text1, 26256]      because he is like what | love |\n [text1, 26309]            . Mother, did you | love |\n [text1, 26314]          love my father as I | love |\n [text1, 26430]       to think of falling in | love |\n [text1, 27986]       she was producing. Her | love |\n [text1, 28301]          , she would fall in | love |\n [text1, 28494]   young dandy who was making | love |\n [text1, 28843]      anything against him. I | love |\n [text1, 28948]                     and I... | love |\n [text1, 28990]                Jim, to be in | love |\n [text1, 29024]               them. To be in | love |\n [text1, 29113]              in at the door, | love |\n [text1, 29268]           day you will be in | love |\n [text1, 29804]       wish you would fall in | love |\n [text1, 29806]          would fall in love. | Love |\n [text1, 29957]         never harm any one I | love |\n [text1, 29969]           Not as long as you | love |\n [text1, 29984]             answer.\" I shall | love |\n [text1, 30639]                it, who is in | love |\n [text1, 31636]       . Dorian Gray falls in | love |\n [text1, 32483]           I was away with my | love |\n [text1, 32685]                I, to take my | love |\n [text1, 33134]            thing he loves. I | love |\n [text1, 33289]    life, your theories about | love |\n [text1, 33873]        gravely.\" They create | love |\n [text1, 34256]             may thrill me. I | love |\n [text1, 34896]            girl. Any one you | love |\n [text1, 35738]      good-night! This bud of | love |\n [text1, 36033]       that about any one you | love |\n [text1, 36037]            you love, Dorian. | Love |\n [text1, 36806]           - oh, my beautiful | love |\n [text1, 36941]  had made me understand what | love |\n [text1, 36946]           love really is. My | love |\n [text1, 36949]                . My love! My | love |\n [text1, 37059]      What could they know of | love |\n [text1, 37141]          to play at being in | love |\n [text1, 37169]         \" You have killed my | love |\n [text1, 37249]         \" you have killed my | love |\n [text1, 37327]             how mad I was to | love |\n [text1, 37417]       little you can know of | love |\n [text1, 37661]       suddenly across me, my | love |\n [text1, 37693]            Kiss me again, my | love |\n [text1, 37767]             to me, because I | love |\n [text1, 37888]       whom one has ceased to | love |\n [text1, 39030]        artist, had given his | love |\n [text1, 39267]         It had taught him to | love |\n [text1, 39523]            marry her, try to | love |\n [text1, 39670]          A faint echo of his | love |\n [text1, 40863]     . His unreal and selfish | love |\n [text1, 43393]   one has killed herself for | love |\n [text1, 43414]        would have made me in | love |\n [text1, 43416]         made me in love with | love |\n [text1, 44144]        romance, passion, and | love |\n [text1, 44202]           all the same. They | love |\n [text1, 45205]           ; she had died for | love |\n [text1, 45210]             love of him, and | love |\n [text1, 45284]  show the supreme reality of | love |\n [text1, 45712]    calling to atom in secret | love |\n [text1, 47056]     had known the reality of | love |\n [text1, 47421]          miseries of life. I | love |\n [text1, 51391]     his own temperament. The | love |\n [text1, 51401]          - for it was really | love |\n [text1, 51440]            tire. It was such | love |\n [text1, 52778]          full of shame. Some | love |\n [text1, 62637]   painted with the images of | love |\n [text1, 63873]       anything about them. I | love |\n [text1, 70156]       there, and a wonderful | love |\n [text1, 74759]  should have fallen madly in | love |\n [text1, 75460]           I believe he is in | love |\n [text1, 75510]           I have not been in | love |\n [text1, 75536]          you men can fall in | love |\n [text1, 76198]        the rejoinder.\" Women | love |\n [text1, 76262]          If we women did not | love |\n [text1, 76594]          long as he does not | love |\n [text1, 80099]         am sick of women who | love |\n [text1, 83059]                  a malady.\"\" | Love |\n [text1, 83455]            as some one says, | love |\n [text1, 83464]            , just as you men | love |\n [text1, 83472]            eyes, if you ever | love |\n [text1, 83501]       then, you never really | love |\n [text1, 86602]            much, but I don't | love |\n [text1, 86704]             \" I wish I could | love |\n [text1, 87228]         Are you very much in | love |\n [text1, 92970]         whom he had lured to | love |\n                                \n secrecy. It seems to           \n : it is the faithless          \n with it, Basil.                \n it is purely a question        \n and death. Suddenly he         \n his playing.\"\"                 \n with somebody, and,            \n , quite charmed,\"              \n . That is one of               \n with?\" asked Lord              \n with love. A _grande           \n . A _grande passion_ is        \n only once in their lives       \n her? Harry, I                  \n her. She is everything         \n , one always begins by         \n her, and I must                \n me. You, who                   \n me! I want to                  \n for Sibyl Vane was a           \n is more than money.            \n him,\" she said                 \n me so much? I                  \n him. I love him                \n him because he is like         \n himself should be. But         \n my father as I love            \n Prince Charming?\" The          \n . Besides, what do             \n was trembling in laughter on   \n with him, and he               \n to her could mean her          \n him.\"\" Why                     \n him. I wish you                \n and play Juliet! To            \n is to surpass one's self       \n flies in through the window    \n yourself. Then you will        \n . Love makes people good       \n makes people good, and         \n , would you?\"                  \n him, I suppose,                \n him for ever!\"                 \n with her, or says              \n with a beautiful girl who      \n in a forest that no            \n out of poetry and to           \n Sibyl Vane. I want             \n , your theories about pleasure \n in our natures. They           \n acting. It is so               \n must be marvellous, and        \n by summer's ripening breath May\n , Dorian. Love is              \n is a more wonderful thing      \n ! - and you freed              \n really is. My love             \n ! My love! Prince              \n ! Prince Charming! Prince      \n such as ours? Take             \n . You have made me             \n ,\" he muttered.                \n . You used to stir             \n you! What a fool               \n , if you say it                \n for you. I think               \n . Don't go away from           \n you better than anything in    \n . Sibyl Vane seemed to         \n to her because he had          \n his own beauty. Would          \n her again. Yes,                \n came back to him.              \n would yield to some higher     \n of you. I wish                 \n with love for the rest         \n for the rest of my             \n .\"\" I was                      \n being dominated. I am          \n of him, and love               \n would always be a sacrament    \n . A wonderful tragic figure    \n or strange affinity? But       \n . When she knew its            \n beautiful things that one can  \n that he bore him -             \n - had nothing in it            \n as Michelangelo had known,     \n might come across his life     \n and death and madness;         \n scandals about other people,   \n that had stirred him to        \n with you,\" she                 \n ,\" cried Lady Narborough       \n for a whole week -             \n with that woman!\"              \n us for our defects.            \n you for your defects,          \n her.\"\" Ah                      \n one. Women who hate            \n ?\"\" An illusion                \n with our ears, just            \n with your eyes, if             \n at all.\"\"                      \n , Mr. Gray,                    \n her.\"\" And                     \n ,\" cried Dorian Gray           \n with him?\" he                  \n him that he was poor           \n\n\n\nbing = get_sentiments(\"bing\")\n\nbook_clean %>% \n  filter(word != c(\"gutenberg\",\"project\")) %>% \n  inner_join(bing, by= 'word') %>% \n  count( sentiment, sort = T)\n\nWarning in word != c(\"gutenberg\", \"project\"): longer object length is not a\nmultiple of shorter object length\n\n\n# A tibble: 2 × 2\n  sentiment     n\n  <chr>     <int>\n1 negative   3161\n2 positive   1986"
  },
  {
    "objectID": "Quanteda-corpus.html",
    "href": "Quanteda-corpus.html",
    "title": "2  Quanteda",
    "section": "",
    "text": "The content in the section has most of the content from Quanteda’s tutorials, with each section adapted to be similar but different. Some topics are not covered in this section such as Wordfish, Regular regression classifier, Topic Models, etc. as to not reproduce a full tutorial series but to show main parts of the library.\nThis section is for the Quanteda tutorials, which has 3 components (object types).\nThe 3 object types:"
  },
  {
    "objectID": "Quanteda-corpus.html#libraries",
    "href": "Quanteda-corpus.html#libraries",
    "title": "2  Quanteda",
    "section": "2.1 Libraries",
    "text": "2.1 Libraries\nHere is the list of Quanteda libraries to install and load in order to follow along with any part of the tutorial.\n\n# install.packages(\"quanteda\")\n# install.packages(\"quanteda.textmodels\") \n# devtools::install_github(\"quanteda/quanteda.corpora\")\n# devtools::install_github(\"kbenoit/quanteda.dictionaries\")\n# install.packages(\"readtext\")\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(quanteda.corpora)\n# used for reading in text data\nlibrary(readtext)\n\nThe readtext package supports: plain text files (.txt), JSON, csv, .tab, .tsv, .xml, pdf, .doc, .docx, etc and can be used to read a book for the Project Gutenberg for text analysis.\nAfter you have properly installed and loaded the libraries, you can access the the data corpus available without the need of reading in a csv file. Load in a corpus from Quanteda package and save it as a variable.\nCorpus available:\n\ndata_corpus_amicus\ndata_corpus_dailnoconf1991\ndata_corpus_EPcoaldebate\ndata_corpus_immigrationnews\ndata_corpus_inaugural most common used one in tutorial\ndata_corpus_irishbudget2010\ndata_corpus_irishbudgets\ndata_corpus_moviereviews\ndata_corpus_sotu State of the Union speeches\ndata_corpus_udhr\ndata_corpus_ukmanifestos\ndata_corpus_ungd2017\n\n\n# to load a corpus and save it as variable name\ncorpus = corpus(data_char_ukimmig2010)"
  },
  {
    "objectID": "Quanteda-corpus.html#corpus",
    "href": "Quanteda-corpus.html#corpus",
    "title": "2  Quanteda",
    "section": "2.2 Corpus",
    "text": "2.2 Corpus\nLoad the corpus data_corpus_ukmanifestos, which is about British election manifestos on immigration and asylum.\n\nbrit_manifesto = corpus(data_corpus_ukmanifestos,\n                        docvars = data.frame(party = names(data_corpus_ukmanifestos)))\n\n\nbrit_manifesto\n\nCorpus consisting of 101 documents and 1 docvar.\nUK_natl_1945_en_Con :\n\"CONSERVATIVE PARTY: 1945  Mr. Churchill's Declaration of Pol...\"\n\nUK_natl_1945_en_Lab :\n\"Labour Party: 1945  Let Us Face the Future: A Declaration of...\"\n\nUK_natl_1945_en_Lib :\n\"LIBERAL MANIFESTO 1945  20 Point Manifesto of the Liberal Pa...\"\n\nUK_natl_1950_en_Con :\n\"CONSERVATIVE PARTY: 1950  This is the Road: The Conservative...\"\n\nUK_natl_1950_en_Lab :\n\"LABOUR PARTY: 1950  Let Us Win Through Together: A Declarati...\"\n\nUK_natl_1950_en_Lib :\n\"LIBERAL PARTY 1950  No Easy Way: Britain's Problems and the ...\"\n\n[ reached max_ndoc ... 95 more documents ]\n\n\n\nhead( summary(brit_manifesto) )\n\n                 Text Types Tokens Sentences               party\n1 UK_natl_1945_en_Con  1752   6679       269 UK_natl_1945_en_Con\n2 UK_natl_1945_en_Lab  1433   5492       234 UK_natl_1945_en_Lab\n3 UK_natl_1945_en_Lib  1208   3729       157 UK_natl_1945_en_Lib\n4 UK_natl_1950_en_Con  2075   8075       366 UK_natl_1950_en_Con\n5 UK_natl_1950_en_Lab  1541   5392       274 UK_natl_1950_en_Lab\n6 UK_natl_1950_en_Lib  1202   3322       136 UK_natl_1950_en_Lib\n\n\n\n2.2.1 docvars\nQuanteda objects keep information associated with documents, these are ‘document level variables’ and are accessed using docvars() function.\n\ninaug_corpus = corpus(data_corpus_inaugural)\n\nhead( docvars(inaug_corpus))\n\n  Year  President FirstName                 Party\n1 1789 Washington    George                  none\n2 1793 Washington    George                  none\n3 1797      Adams      John            Federalist\n4 1801  Jefferson    Thomas Democratic-Republican\n5 1805  Jefferson    Thomas Democratic-Republican\n6 1809    Madison     James Democratic-Republican\n\n\nTo extract docvars variables use the field argument or use the $ like you normally use on a dataframe.\n\ndocvars( inaug_corpus, field = 'Year')\n\n [1] 1789 1793 1797 1801 1805 1809 1813 1817 1821 1825 1829 1833 1837 1841 1845\n[16] 1849 1853 1857 1861 1865 1869 1873 1877 1881 1885 1889 1893 1897 1901 1905\n[31] 1909 1913 1917 1921 1925 1929 1933 1937 1941 1945 1949 1953 1957 1961 1965\n[46] 1969 1973 1977 1981 1985 1989 1993 1997 2001 2005 2009 2013 2017 2021\n\n\nCreate or update docvars, in this example of creating a column for Century.\n\nfloor_ = floor(docvars(inaug_corpus, field = 'Year') / 100)+1\n\ndocvars(inaug_corpus, field = 'Century') = floor_\n\nhead(docvars(inaug_corpus))\n\n  Year  President FirstName                 Party Century\n1 1789 Washington    George                  none      18\n2 1793 Washington    George                  none      18\n3 1797      Adams      John            Federalist      18\n4 1801  Jefferson    Thomas Democratic-Republican      19\n5 1805  Jefferson    Thomas Democratic-Republican      19\n6 1809    Madison     James Democratic-Republican      19\n\n\n\n\n2.2.2 subset\nThis section is about using the corpus_subset() function on all docvars.\n\n#--- get the Inaugural Speeches\n# inaug_corpus\n\n#--- look at the docvars\n# head( docvars(inaug_corpus))\n\n# now we subset the corpus for speeches after 1990\ninaug_corpus_subset1990s = corpus_subset(inaug_corpus, Year >= 1990)\n\n# check the number of documents \nhead(inaug_corpus_subset1990s, 3)\n\nCorpus consisting of 3 documents and 5 docvars.\n1993-Clinton :\n\"My fellow citizens, today we celebrate the mystery of Americ...\"\n\n1997-Clinton :\n\"My fellow citizens: At this last presidential inauguration o...\"\n\n2001-Bush :\n\"President Clinton, distinguished guests and my fellow citize...\"\n\n\nIf you want only specific US Presidents. Note that there are 2 data objects that appear when you type ‘President’, presidents from the {datasets} package and presidential from {ggplot2} package.\n\n# select specific US Presidents\nselected_presidents = c('Obama','Clinton','Carter')\n\ndemocrat_corpus_subset =  corpus_subset(inaug_corpus,\n                                        President %in% selected_presidents\n                                        )\n\n\ndemocrat_corpus_subset\n\nCorpus consisting of 5 documents and 5 docvars.\n1977-Carter :\n\"For myself and for our Nation, I want to thank my predecesso...\"\n\n1993-Clinton :\n\"My fellow citizens, today we celebrate the mystery of Americ...\"\n\n1997-Clinton :\n\"My fellow citizens: At this last presidential inauguration o...\"\n\n2009-Obama :\n\"My fellow citizens: I stand here today humbled by the task b...\"\n\n2013-Obama :\n\"Vice President Biden, Mr. Chief Justice, Members of the Unit...\"\n\n\n\n\n2.2.3 reshape\nYou can reshape the document paragraphs to sentences, which can be restored even after being modified by functions.\n\nUN_corpus = corpus(data_corpus_ungd2017)\n\n# 196 documents, 7 docvars\nhead(UN_corpus)\n\nCorpus consisting of 6 documents and 7 docvars.\nAfghanistan :\n\"As I stand here before the General Assembly today, I am remi...\"\n\nAngola :\n\"On behalf of the Government of the Republic of Angola, allow...\"\n\nAlbania :\n\"How many times has it happened that humankind has been confr...\"\n\nAndorra :\n\"I would like to begin by congratulating the President, Mr. M...\"\n\nUnited Arab Emirates :\n\"I would like to begin by congratulating the President on his...\"\n\nArgentina :\n\"I am very honoured and very happy to be here today represent...\"\n\n\nNow change the document into sentences\n\nUN_corpus_sentences = corpus_reshape(UN_corpus, to= 'sentences')\n\nhead(UN_corpus_sentences)\n\nCorpus consisting of 6 documents and 7 docvars.\nAfghanistan.1 :\n\"As I stand here before the General Assembly today, I am remi...\"\n\nAfghanistan.2 :\n\"Shaped by the Great Depression and tempered by the carnage o...\"\n\nAfghanistan.3 :\n\"The United Nations, the International Monetary Fund, the Wor...\"\n\nAfghanistan.4 :\n\"There can be little doubt that today the scale, scope and sp...\"\n\nAfghanistan.5 :\n\"But future historians will judge those institutions on how t...\"\n\nAfghanistan.6 :\n\"As global leaders, we seek certainty and familiarity in the ...\"\n\n# there is now 16806 number of documents\n\nTo change back\n\nUN_corpus_doc = corpus_reshape(UN_corpus_sentences, to= 'documents')\n\n\n\n2.2.4 segment\nYou can extract segments of text and tags from documents, the use-case for this is analyzing documents or transcripts separately.\nDocument sections\nThe created mini text document that has 6 lines, which has section that uses “##” as a pattern to organize the text into new lines. Here is what the text file contains.\n\n##INTRO This is the introduction.\n##DOC1 This is the first document.  Second sentence in Doc 1.\n##DOC3    Third document starts here.  End of third document.\n##INTRO                                               Document\n##NUMBER                                      Two starts before\n##NUMBER                                                 Three.\n\n\nlibrary(readr)\n\n# read in a simple text file as shown above\ntxt_file = readr::read_file('./doctext.txt')\n\n# need to convert it to a corpus object\ntxt_file_corpus = corpus(txt_file)\n\n# provide the pattern of ## in order for it to be separated into new lines\ntxt_seg = corpus_segment(txt_file_corpus, pattern = \"##*\")\ntxt_seg\n\nCorpus consisting of 6 documents and 1 docvar.\ntext1.1 :\n\"This is the introduction.\"\n\ntext1.2 :\n\"This is the first document.  Second sentence in Doc 1.\"\n\ntext1.3 :\n\"Third document starts here.  End of third document.\"\n\ntext1.4 :\n\"Document\"\n\ntext1.5 :\n\"Two starts before\"\n\ntext1.6 :\n\"Three.\"\n\n\nBy providing a pattern value for the corpus_segment() you can do regular expressions for removing words and symbols.\n\n# transform text into corpus\ncorp_speeches = corpus(\"Mr. Samwell Tarly: Text Analysis.\n                        Mx. Jones: More text analysis?\n                        Mr. Samwell Tarly: we will do it again, and again.\")\n\n# use corpus segment with regular expression pattern matching\ncorp_speakers = corpus_segment(corp_speeches, \n                                pattern = \"\\\\b[A-Z].+\\\\s[A-Z][a-z]+:\", \n                                valuetype = \"regex\")\n\n# bind the rows together\ncbind(docvars(corp_speakers), text = as.character(corp_speakers))\n\n                   pattern                            text\ntext1.1 Mr. Samwell Tarly:                  Text Analysis.\ntext1.2         Mx. Jones:             More text analysis?\ntext1.3 Mr. Samwell Tarly: we will do it again, and again."
  },
  {
    "objectID": "Quanteda-corpus.html#tokens",
    "href": "Quanteda-corpus.html#tokens",
    "title": "2  Quanteda",
    "section": "2.3 Tokens",
    "text": "2.3 Tokens\nTokens segments texts in a corpus (words or sentences) by word boundaries.\n\n# step 1 - load in corpus\nsotu_corpus = corpus(data_corpus_sotu)\n\n# step 2 - pass in the corpus into tokens function\n# using the tokens() you can remove punctuation and numbers\n\nsotu_corpus_tokens = tokens(sotu_corpus, \n                            remove_punct = TRUE,\n                            remove_symbols = TRUE,\n                            remove_numbers = TRUE,\n                            remove_url = TRUE\n                            )\n\nhead(sotu_corpus_tokens, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"Senate\"         \n [5] \"and\"             \"House\"           \"of\"              \"Representatives\"\n [9] \"I\"               \"embrace\"         \"with\"            \"great\"          \n[ ... and 1,073 more ]\n\nWashington-1790b :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"Senate\"         \n [5] \"and\"             \"House\"           \"of\"              \"Representatives\"\n [9] \"In\"              \"meeting\"         \"you\"             \"again\"          \n[ ... and 1,388 more ]\n\nWashington-1791 :\n [1] \"Fellow-Citizens\" \"of\"              \"the\"             \"Senate\"         \n [5] \"and\"             \"House\"           \"of\"              \"Representatives\"\n [9] \"In\"              \"vain\"            \"may\"             \"we\"             \n[ ... and 2,285 more ]\n\n\n\n2.3.1 kwic\nKeyword-in-contexts (kwic) is the method used to search for keywords in corpus documents.\n\n#-- continue using the tokenized corpus of State of the Union\n# sotu_corpus_tokens\n\n# keyword to search the corpus follows the pattern argument\n# our keyword here will be 'love'\n# save kwic in a variable\n\nkeyword_search =  kwic(sotu_corpus_tokens, pattern = 'love')\n\nhead(keyword_search)\n\nKeyword-in-context with 6 matches.                                                                    \n [Washington-1790b, 1327]        wisdom and animated by the | love |\n  [Washington-1793, 1122]  equity proceeding from a sincere | love |\n  [Washington-1795, 1011] acknowledgment to Heaven and pure | love |\n    [Jefferson-1801, 923]      of happiness educated in the | love |\n     [Jefferson-1807, 37]     earlier period than usual The | love |\n   [Jefferson-1808, 2621] unshaken by difficulties in their | love |\n                                       \n of your country In whatever           \n of peace and a liberality             \n to our country to unite               \n of order habituated to self-government\n of peace so much cherished            \n of liberty obedience to law           \n\n\nYou can search for multiple words by using a vector.\n\nsearch_words = c('love','science')\n\nsearched_words = kwic(sotu_corpus_tokens, pattern = search_words)\n\nhead(searched_words)\n\nKeyword-in-context with 6 matches.                                                                       \n   [Washington-1790, 641]   patronage than the promotion of | science |\n [Washington-1790b, 1327]        wisdom and animated by the |  love   |\n  [Washington-1793, 1122]  equity proceeding from a sincere |  love   |\n  [Washington-1795, 1011] acknowledgment to Heaven and pure |  love   |\n  [Washington-1796, 1910]               of our youth in the | science |\n    [Jefferson-1801, 923]      of happiness educated in the |  love   |\n                                       \n and literature Knowledge is in        \n of your country In whatever           \n of peace and a liberality             \n to our country to unite               \n of government In a republic           \n of order habituated to self-government\n\n\nFor multi-word expressions, use the phrase() function after the pattern argument.\n\nus_searchword = kwic(sotu_corpus_tokens, pattern =  phrase(\"United States\"))\n\nhead(us_searchword)\n\nKeyword-in-context with 6 matches.                                                                              \n    [Washington-1790, 47:48]      to the Constitution of the | United States |\n  [Washington-1790, 397:398] aggressors The interests of the | United States |\n  [Washington-1790, 520:521]     weights and measures of the | United States |\n  [Washington-1790, 956:957]  character and interests of the | United States |\n [Washington-1790b, 520:521]              on the part of the | United States |\n [Washington-1790b, 743:744]     and security enjoyed by the | United States |\n                                   \n of which official information has \n require that our intercourse with \n is an object of great             \n are so obviously so deeply        \n renewed their violences with fresh\n reminds us at the same            \n\n\nTo view all of the keyword search results in a RStudio window.\n\n# view(us_searchword)\n\n\n\n2.3.2 select tokens\nYou can remove tokens that you do not want or interest in, this can be either on its own or in combination with stopwords() function.\nSo far we have tokenized the State of the Union corpus and did some keyword searched, but the corpus still has stopwords inside the text, to remove them we use the function tokens_select() and pass in the stopwords along with the language parameter.\n\n#-- continue using sotu corpus\n\nsotu_corpus_tokens_clean = tokens_select(sotu_corpus_tokens,\n              pattern =  stopwords('en'), # 'en' for English\n              selection = 'remove'\n              )\n\nhead(sotu_corpus_tokens_clean, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n [1] \"Fellow-Citizens\" \"Senate\"          \"House\"           \"Representatives\"\n [5] \"embrace\"         \"great\"           \"satisfaction\"    \"opportunity\"    \n [9] \"now\"             \"presents\"        \"congratulating\"  \"present\"        \n[ ... and 488 more ]\n\nWashington-1790b :\n [1] \"Fellow-Citizens\" \"Senate\"          \"House\"           \"Representatives\"\n [5] \"meeting\"         \"feel\"            \"much\"            \"satisfaction\"   \n [9] \"able\"            \"repeat\"          \"congratulations\" \"favorable\"      \n[ ... and 619 more ]\n\nWashington-1791 :\n [1] \"Fellow-Citizens\" \"Senate\"          \"House\"           \"Representatives\"\n [5] \"vain\"            \"may\"             \"expect\"          \"peace\"          \n [9] \"Indians\"         \"frontiers\"       \"long\"            \"lawless\"        \n[ ... and 1,058 more ]\n\n#---- equivalent function call\n#  tokens_remove( sotu_corpus_tokens, pattern= stopwords('en'), padding= FALSE )\n\nFor very specific words that interest you in token selection, you can pass in a vector of those words.\n\nspecific_words = c('love','scien*','reason')\n\nspecific_tokens_select = tokens_select(sotu_corpus_tokens,\n                                       pattern = specific_words,\n                                       padding = FALSE\n                                       )\n\n# Tokens consisting of 241 documents and 6 docvars.\nhead(specific_tokens_select)\n\nTokens consisting of 6 documents and 6 docvars.\nWashington-1790 :\n[1] \"reason\"  \"science\"\n\nWashington-1790b :\n[1] \"love\"\n\nWashington-1791 :\ncharacter(0)\n\nWashington-1792 :\n[1] \"reason\"\n\nWashington-1793 :\n[1] \"reason\" \"love\"  \n\nWashington-1794 :\n[1] \"reason\" \"reason\"\n\n\nTo see the words that surround the selected token, use the window argument, here the window is 5 words.\n\nwindow_token_select = tokens_select(sotu_corpus_tokens, \n              pattern = specific_words, \n              padding = F, \n              window = 5\n              )\n\nhead(window_token_select, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n [1] \"regard\"    \"to\"        \"economy\"   \"There\"     \"was\"       \"reason\"   \n [7] \"to\"        \"hope\"      \"that\"      \"the\"       \"pacific\"   \"patronage\"\n[ ... and 10 more ]\n\nWashington-1790b :\n [1] \"wisdom\"   \"and\"      \"animated\" \"by\"       \"the\"      \"love\"    \n [7] \"of\"       \"your\"     \"country\"  \"In\"       \"whatever\"\n\nWashington-1791 :\ncharacter(0)\n\n\n\n\n2.3.3 compund tokens\nSimilar to what we already did, there is a tokens_compound() function that uses a vector for tokens to search. This will look just like the kwic() function.\n\n#---- kwic() \nsearched_words2 = c(\"americ*\", \"american people\")\n\namerica_kwic = kwic(sotu_corpus_tokens, pattern = searched_words2)\n\nhead(america_kwic)\n\nKeyword-in-context with 6 matches.                                                                          \n  [Washington-1790b, 67]            by a considerable rise of | American |\n   [Washington-1794, 22]               of Heaven by which the | American |\n [Washington-1794, 1988]     over that precious depository of | American |\n  [Washington-1795, 732]         nations of Europe with their | American |\n  [Washington-1796, 773]         the protection and relief of | American |\n      [Adams-1797, 1583] obligations The numerous captures of | American |\n                                   \n stock abroad as well as           \n people became a nation when       \n happiness the Constitution of the \n dependencies have been involved in\n sea-men agents were appointed one \n vessels by the cruisers of        \n\n\n\n#---- tokens_compound\n\namerica_token_compound = tokens_compound(sotu_corpus_tokens,\n                pattern = phrase(searched_words2) \n                )\n\n# head(america_token_compound)\nhead( kwic(america_token_compound, pattern = searched_words2) )\n\nKeyword-in-context with 6 matches.                                                                               \n  [Washington-1790b, 67]            by a considerable rise of |    American    \n   [Washington-1794, 22]               of Heaven by which the | American_people\n [Washington-1794, 1987]     over that precious depository of |    American    \n  [Washington-1795, 732]         nations of Europe with their |    American    \n  [Washington-1796, 773]         the protection and relief of |    American    \n      [Adams-1797, 1583] obligations The numerous captures of |    American    \n                                     \n | stock abroad as well as           \n | became a nation when we           \n | happiness the Constitution of the \n | dependencies have been involved in\n | sea-men agents were appointed one \n | vessels by the cruisers of        \n\n\n\n\n2.3.4 n-grams\nYou can make n-grams in any length from tokens using the tokens_ngrams() which makes a sequence of tokens.\n\n# -- pass in the sotu corpus tokens object\n#    n-gram will be from 2 to 4\n\nsotu_corpus_ngrams = tokens_ngrams(sotu_corpus_tokens, n= 2:4)\n\n# for simplicity, the 1st document is shown\n# 1st document, 20 of the docvars n-grams\n\nhead(sotu_corpus_ngrams[[1]], 20)\n\n [1] \"Fellow-Citizens_of\" \"of_the\"             \"the_Senate\"        \n [4] \"Senate_and\"         \"and_House\"          \"House_of\"          \n [7] \"of_Representatives\" \"Representatives_I\"  \"I_embrace\"         \n[10] \"embrace_with\"       \"with_great\"         \"great_satisfaction\"\n[13] \"satisfaction_the\"   \"the_opportunity\"    \"opportunity_which\" \n[16] \"which_now\"          \"now_presents\"       \"presents_itself\"   \n[19] \"itself_of\"          \"of_congratulating\" \n\n# head(sotu_corpus_ngrams, 3)\n\nYou can skip n-grams\n\nsotu_corpus_ngrams_skip = tokens_ngrams(sotu_corpus_tokens,\n                                        n= 2,\n                                        skip = 1:2\n                                        )\n\n# notice the document returns [1][4] ...\n\nhead(sotu_corpus_ngrams_skip, 2)\n\nTokens consisting of 2 documents and 6 docvars.\nWashington-1790 :\n [1] \"Fellow-Citizens_the\"    \"Fellow-Citizens_Senate\" \"of_Senate\"             \n [4] \"of_and\"                 \"the_and\"                \"the_House\"             \n [7] \"Senate_House\"           \"Senate_of\"              \"and_of\"                \n[10] \"and_Representatives\"    \"House_Representatives\"  \"House_I\"               \n[ ... and 2,153 more ]\n\nWashington-1790b :\n [1] \"Fellow-Citizens_the\"    \"Fellow-Citizens_Senate\" \"of_Senate\"             \n [4] \"of_and\"                 \"the_and\"                \"the_House\"             \n [7] \"Senate_House\"           \"Senate_of\"              \"and_of\"                \n[10] \"and_Representatives\"    \"House_Representatives\"  \"House_In\"              \n[ ... and 2,783 more ]\n\n\nSelective ngrams\nSelecting ngrams based on a keyword, phrase or a vector of words is done by using the tokens_compound() and tokens_select().\n\n#-- select phrases that have 'not'\n\nphrase_not = c(\"not *\",\"not_*\")\n\nsotu_not_ngrams = tokens_compound(sotu_corpus_tokens,\n                pattern = phrase(phrase_not)\n                )\n\nsotu_not_ngrams_select = tokens_select(sotu_not_ngrams, \n                                       # need to include again\n                                       pattern = phrase(phrase_not)\n                                       )\n\nhead(sotu_not_ngrams_select, 3)\n\nTokens consisting of 3 documents and 6 docvars.\nWashington-1790 :\n[1] \"not_but\"     \"not_only\"    \"not_I\"       \"not_forbear\"\n\nWashington-1790b :\n[1] \"not_only\"      \"not_fail\"      \"not_less\"      \"not_be\"       \n[5] \"not_overlook\"  \"not_less\"      \"not_think\"     \"not_desirable\"\n[9] \"not_merely\"   \n\nWashington-1791 :\n[1] \"not_fail\"    \"not_only\"    \"not_forbear\" \"not_fail\""
  },
  {
    "objectID": "Quanteda-corpus.html#document-feature-matrix",
    "href": "Quanteda-corpus.html#document-feature-matrix",
    "title": "2  Quanteda",
    "section": "2.4 document-feature-matrix",
    "text": "2.4 document-feature-matrix\nA document-feature-matrix (dfm) is made from tokens objects. Unlike all the work previously the objects returned were documents in a row with docvars for each tokens operation performed. A dfm returns a dataframe like object with features and docvars.\n\n# -- continue to use the sotu tokens object where punctuation etc. was removed\n# sotu_corpus_tokens\n\nsotu_corpus_dfm = dfm(sotu_corpus_tokens)\n\nhead(sotu_corpus_dfm)\n\nDocument-feature matrix of: 6 documents, 29,008 features (97.48% sparse) and 6 docvars.\n                  features\ndocs               fellow-citizens  of the senate and house representatives  i\n  Washington-1790                1  69  97      2  41     3               3 11\n  Washington-1790b               1  89 122      2  45     3               3  8\n  Washington-1791                1 159 242      3  73     3               3  6\n  Washington-1792                1 139 195      2  56     3               3 21\n  Washington-1793                1 132 180      2  49     3               3 12\n  Washington-1794                1 187 273      2  86     4               3 18\n                  features\ndocs               embrace with\n  Washington-1790        1   11\n  Washington-1790b       0   15\n  Washington-1791        0   27\n  Washington-1792        0   20\n  Washington-1793        0   24\n  Washington-1794        0   17\n[ reached max_nfeat ... 28,998 more features ]\n\n\nCommon functions to use with a document-feature-matrix:\n\ndocnames( )\nfeatnames( )\nrowSums( )\ncolSums( )\n\nTo get the most frequent features can be retrieved by using topfeatures()\n\n#-- this corpus dfm has stopwords included\n# topfeatures(sotu_corpus_dfm)\n\n#-- to use the cleaned corpus need to make it a dfm\n\nsotu_corpus_tokens_clean_dfm = dfm(sotu_corpus_tokens_clean)\n\ntopfeatures(sotu_corpus_tokens_clean_dfm)\n\ngovernment     states   congress     united        can       year     people \n      7444       6942       5795       5181       4845       4617       4286 \n      upon    country       must \n      4229       3619       3600 \n\n\nTo get a proportion of a feature within documents total count, use the dfm_weight(scheme='prop'), which is a relative frequency. The scheme by default is ‘count’.\n\n# dfm_weight(sotu_corpus_dfm, scheme = 'prop')\n\ndfm_weight(sotu_corpus_tokens_clean_dfm, scheme = 'prop')\n\nDocument-feature matrix of: 241 documents, 28,840 features (94.28% sparse) and 6 docvars.\n                  features\ndocs               fellow-citizens      senate       house representatives\n  Washington-1790     0.0020000000 0.004000000 0.006000000     0.006000000\n  Washington-1790b    0.0015847861 0.003169572 0.004754358     0.004754358\n  Washington-1791     0.0009345794 0.002803738 0.002803738     0.002803738\n  Washington-1792     0.0010427529 0.002085506 0.003128259     0.003128259\n  Washington-1793     0.0011098779 0.002219756 0.003329634     0.003329634\n  Washington-1794     0.0007485030 0.001497006 0.002994012     0.002245509\n                  features\ndocs               embrace       great satisfaction  opportunity         now\n  Washington-1790    0.002 0.008000000  0.004000000 0.0020000000 0.002000000\n  Washington-1790b   0     0.006339144  0.001584786 0            0.003169572\n  Washington-1791    0     0            0.002803738 0.0009345794 0          \n  Washington-1792    0     0            0.003128259 0            0.002085506\n  Washington-1793    0     0            0           0.0011098779 0.002219756\n  Washington-1794    0     0.000748503  0           0.0007485030 0.002245509\n                  features\ndocs               presents\n  Washington-1790     0.002\n  Washington-1790b    0    \n  Washington-1791     0    \n  Washington-1792     0    \n  Washington-1793     0    \n  Washington-1794     0    \n[ reached max_ndoc ... 235 more documents, reached max_nfeat ... 28,830 more features ]\n\n\nGet the weight of dfm by tf-idf frequency inverse document frequency\n\ndfm_tfidf(sotu_corpus_tokens_clean_dfm, \n          scheme_df = 'inverse', \n          scheme_tf = 'count'\n          )\n\nDocument-feature matrix of: 241 documents, 28,840 features (94.28% sparse) and 6 docvars.\n                  features\ndocs               fellow-citizens    senate     house representatives\n  Washington-1790        0.5246845 0.1663279 0.2044495       0.4394658\n  Washington-1790b       0.5246845 0.1663279 0.2044495       0.4394658\n  Washington-1791        0.5246845 0.2494919 0.2044495       0.4394658\n  Washington-1792        0.5246845 0.1663279 0.2044495       0.4394658\n  Washington-1793        0.5246845 0.1663279 0.2044495       0.4394658\n  Washington-1794        0.5246845 0.1663279 0.2725993       0.4394658\n                  features\ndocs                 embrace       great satisfaction opportunity         now\n  Washington-1790  0.7485486 0.021760342    0.6351181  0.08098705 0.005440086\n  Washington-1790b 0         0.021760342    0.3175591  0          0.010880171\n  Washington-1791  0         0              0.9526772  0.08098705 0          \n  Washington-1792  0         0              0.9526772  0          0.010880171\n  Washington-1793  0         0              0          0.08098705 0.010880171\n  Washington-1794  0         0.005440086    0          0.08098705 0.016320257\n                  features\ndocs                presents\n  Washington-1790  0.5559422\n  Washington-1790b 0        \n  Washington-1791  0        \n  Washington-1792  0        \n  Washington-1793  0        \n  Washington-1794  0        \n[ reached max_ndoc ... 235 more documents, reached max_nfeat ... 28,830 more features ]\n\n\n\n2.4.1 dfm_keep\nYou can select features from a dfm using dfm_select() and dfm_keep(), keep the number of features at least the number of times.\n\n# -- continue with sotu tokens dfm\n# sotu_corpus_tokens_clean_dfm\n\nsotu_clean_dfm_keep = dfm_keep(sotu_corpus_tokens_clean_dfm,\n                               min_nchar= 5)\n\nhead(sotu_clean_dfm_keep)\n\nDocument-feature matrix of: 6 documents, 26,948 features (97.87% sparse) and 6 docvars.\n                  features\ndocs               fellow-citizens senate house representatives embrace great\n  Washington-1790                1      2     3               3       1     4\n  Washington-1790b               1      2     3               3       0     4\n  Washington-1791                1      3     3               3       0     0\n  Washington-1792                1      2     3               3       0     0\n  Washington-1793                1      2     3               3       0     0\n  Washington-1794                1      2     4               3       0     1\n                  features\ndocs               satisfaction opportunity presents congratulating\n  Washington-1790             2           1        1              1\n  Washington-1790b            1           0        0              0\n  Washington-1791             3           1        0              0\n  Washington-1792             3           0        0              0\n  Washington-1793             0           1        0              0\n  Washington-1794             0           1        0              0\n[ reached max_nfeat ... 26,938 more features ]\n\n\nSee the topfeatures for the min characters\n\ntopfeatures( sotu_clean_dfm_keep)\n\ngovernment     states   congress     united     people    country      great \n      7444       6942       5795       5181       4286       3619       3468 \n    public   american      years \n      3400       2928       2699 \n\n\nIf the number of features is below a number threshold that you want, you can use the min_termfreq = n to drop the features below n. In this example we use 10, so any feature that appears less than 10 times in all of the document will be removed. If max_docfreq = 0.1, then features more than 10% of the documents will be removed.\n\n#  an option is to include docfreq_type= 'prop'\n#  for relative proportion\n\nsotu_clean_minfreq = dfm_trim(sotu_corpus_tokens_clean_dfm,\n                              min_termfreq = 10)\n\nhead(sotu_clean_minfreq)\n\nDocument-feature matrix of: 6 documents, 8,248 features (92.71% sparse) and 6 docvars.\n                  features\ndocs               fellow-citizens senate house representatives embrace great\n  Washington-1790                1      2     3               3       1     4\n  Washington-1790b               1      2     3               3       0     4\n  Washington-1791                1      3     3               3       0     0\n  Washington-1792                1      2     3               3       0     0\n  Washington-1793                1      2     3               3       0     0\n  Washington-1794                1      2     4               3       0     1\n                  features\ndocs               satisfaction opportunity now presents\n  Washington-1790             2           1   1        1\n  Washington-1790b            1           0   2        0\n  Washington-1791             3           1   0        0\n  Washington-1792             3           0   2        0\n  Washington-1793             0           1   2        0\n  Washington-1794             0           1   3        0\n[ reached max_nfeat ... 8,238 more features ]\n\n\n\n\n2.4.2 dfm group\nYou can merge documents based on a vector and then takes the sum of feature frequencies.\n\n#-- Inaugural Speech corpus\n# we skipped the corpus() then tokens() step here\n# as you can find the corpus by typing data_<tab>\n\ninaug_tokens = tokens(data_corpus_inaugural)\n\ninaug_dfm = dfm(inaug_tokens)\n# head(inaug_dfm)\n\n# group by Party\ndfm_group_party = dfm_group(inaug_dfm, groups = Party)\n\nhead(dfm_group_party)\n\nDocument-feature matrix of: 6 documents, 9,439 features (66.93% sparse) and 1 docvar.\n                       features\ndocs                    fellow-citizens   of  the senate  and house\n  Democratic                          3 1994 2742      2 1728     4\n  Democratic-Republican              10  945 1416      0  640     0\n  Federalist                          3  140  163      1  130     0\n  none                                1   82  129      1   50     2\n  Republican                          9 3055 4408      5 2386     4\n  Whig                               13  964 1325      6  472     1\n                       features\ndocs                    representatives  : among vicissitudes\n  Democratic                          3 54    25            3\n  Democratic-Republican               2  1    16            1\n  Federalist                          2  0     4            0\n  none                                2  2     1            1\n  Republican                          6 86    52            0\n  Whig                                4  1    10            0\n[ reached max_nfeat ... 9,429 more features ]\n\ndocvars(dfm_group_party)\n\n                  Party\n1            Democratic\n2 Democratic-Republican\n3            Federalist\n4                  none\n5            Republican\n6                  Whig\n\n\n\n\n2.4.3 FCM\nFeature co-occurrence matrix acts similar to dfm. You can construct a FCM from a DFM or a tokens object using fcm(). topfeatures() returns the most frequently co-occuring words.\n\n# download news corpus\ncorp_news = download(\"data_corpus_guardian\")\n\nhead(corp_news)\n\nCorpus consisting of 4 documents and 9 docvars.\ntext136751 :\n\"London masterclass on climate change | Do you want to unders...\"\n\ntext118588 :\n\"As colourful fish were swimming past him off the Greek coast...\"\n\ntext45146 :\n\"FTSE 100 | -101.35 | 6708.35 | FTSE All Share | -58.11 | 360...\"\n\ntext93623 :\n\"Australia's education minister, Christopher Pyne, has vowed ...\"\n\n\n\nnews_tokens = tokens(corp_news,\n                    remove_punct = T,\n                    remove_separators = T,\n                    remove_url = T)\n\nnews_dfm = dfm(news_tokens) \n\nnews_dfm = dfm_remove(news_dfm , \n                      pattern= c(stopwords(\"en\"), \"*-time\", \"updated-*\", \"gmt\", \"bst\",\"|\"))\n\nnews_dfm_trim = dfm_trim(news_dfm, min_termfreq = 100)\n\ntopfeatures(news_dfm_trim)\n\n      said     people        one        new       also         us        can \n     28412      11168       9879       8024       7901       7090       6972 \ngovernment       year       last \n      6821       6570       6335"
  },
  {
    "objectID": "Quanteda-corpus.html#stat-analysis",
    "href": "Quanteda-corpus.html#stat-analysis",
    "title": "2  Quanteda",
    "section": "2.5 Stat Analysis",
    "text": "2.5 Stat Analysis\n\nsotu_statfreq = textstat_frequency(sotu_corpus_tokens_clean_dfm,\n                   n= 5,\n                   groups = President)\n\nhead( sotu_statfreq )\n\n     feature frequency rank docfreq  group\n1     states       194    1       8  Adams\n2     united       175    2       8  Adams\n3       upon       165    3       6  Adams\n4   congress       143    4       8  Adams\n5        may       123    5       8  Adams\n6 government       185    1       4 Arthur\n\n\n\n2.5.1 lexical diversity\ntextstat_lexdiv() calculates various lexical diversity measures based on the number of unique types of tokens and the length of a document. It is useful, for instance, for analyzing speakers’ or writers’ linguistic skills, or the complexity of ideas expressed in documents.\n\ninaug_tokens_dfm = dfm(inaug_tokens) %>% \n  dfm_remove(stopwords('en'))\n\n# inaug_tokens_dfm\n\n\ninaug_lexdiv = textstat_lexdiv( inaug_tokens_dfm ) \n\nhead( inaug_lexdiv )\n\n         document       TTR\n1 1789-Washington 0.7806748\n2 1793-Washington 0.9354839\n3      1797-Adams 0.6542056\n4  1801-Jefferson 0.7293973\n5  1805-Jefferson 0.6726014\n6    1809-Madison 0.8326996\n\n\n\nplot(inaug_lexdiv$TTR, type = \"l\", xaxt = \"n\", xlab = NULL, ylab = \"TTR\")\ngrid()\naxis(1, at = seq_len(nrow(inaug_lexdiv)), labels = inaug_dfm$President )\n\n\n\n\n\n\n2.5.2 document feature similarity\ntextstat_dist() calculates similarities of documents or features for various measures. Its output is compatible with R’s dist(), so hierarchical clustering can be performed without any transformation.\n\ninaug_dfm_dist = as.dist( inaug_tokens_dfm )  \n\nWarning in as.dist.default(inaug_tokens_dfm): non-square matrix\n\ninaug_clustering = hclust( inaug_dfm_dist )\n\nplot( inaug_clustering, xlab= \"Distance\", ylab=\"\")\n\n\n\n\n\n\n2.5.3 relative frequency analysis\nAlso known as keyness, Keyness is a signed two-by-two association scores originally implemented in WordSmith to identify frequent words in documents in a target and reference group.\n\nsotu_keyness = textstat_keyness( sotu_corpus_dfm ) \n\ntextplot_keyness( sotu_keyness)\n\n\n\n\n\n\n2.5.4 collocation analysis\nA collocation analysis allows us to identify contiguous collocations of words. One of the most common types of multi-word expressions are proper names, which can be identified simply based on capitalization in English texts.\nExample from Quanteda, not run here.\n\ncorp_news <- download(\"data_corpus_guardian\")\n\n\ntoks_news <- tokens(corp_news, remove_punct = TRUE)\n\ntstat_col_caps <- tokens_select(toks_news, pattern = \"^[A-Z]\", \n                                valuetype = \"regex\", \n                                case_insensitive = FALSE, \n                                padding = TRUE) %>% \n                  textstat_collocations(min_count = 100)\n                  \nhead(tstat_col_caps, 20)"
  },
  {
    "objectID": "Quanteda-corpus.html#advanced-operations",
    "href": "Quanteda-corpus.html#advanced-operations",
    "title": "2  Quanteda",
    "section": "2.6 Advanced Operations",
    "text": "2.6 Advanced Operations\nWe can compute the similarities between authors by grouping their documents and comparing them with all other authors.\n\ntwitter_df = read_csv2(\"./TorontoTweetsText.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 7985 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (1): text\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(twitter_df)\n\n# A tibble: 6 × 1\n  text                                                                          \n  <chr>                                                                         \n1 \"📌 @mlleperez\\n#Toronto #TheSix #EstudiaenCanadá🇨🇦 https://t.co/FzjSa3PhDv\"  \n2 \"September marks a huge milestone @DistilleryTO …190 years old! Come explore …\n3 \"The average home price rising again in Toronto.\\nhttps://t.co/NjHh7B5Odo\\n#t…\n4 \"Sault News: Real estate prices expected to remain stable | CTV News\\nhttps:/…\n5 \"Saturday vibes! #Toronto https://t.co/RKEosXP6X7\"                            \n6 \"#Toronto head over to #BloorWest to support #Ukraine️ this afternoon! https:/…\n\n\nconstruct a corpus\n\ntwitter_corpus = corpus(twitter_df)\n\nhead(twitter_corpus)\n\nCorpus consisting of 6 documents.\ntext1 :\n\"📌 @mlleperez #Toronto #TheSix #EstudiaenCanadá🇨🇦 https://t.c...\"\n\ntext2 :\n\"September marks a huge milestone @DistilleryTO …190 years ol...\"\n\ntext3 :\n\"The average home price rising again in Toronto. https://t.co...\"\n\ntext4 :\n\"Sault News: Real estate prices expected to remain stable | C...\"\n\ntext5 :\n\"Saturday vibes! #Toronto https://t.co/RKEosXP6X7\"\n\ntext6 :\n\"#Toronto head over to #BloorWest to support #Ukraine️ this a...\"\n\n\nConstruct a document-feature matrix, and remove tags, links, and English stopwords.\n\ntwitter_dfm = twitter_corpus %>% \n  # tokenize and clean text\n  tokens(remove_punct = TRUE,\n         remove_symbols = TRUE,\n         remove_url = TRUE,\n         remove_separators = TRUE) %>% \n  # make it into a dfm\n  dfm() %>% \n  # remove stopwords\n  dfm_remove(stopwords(\"en\")) %>% \n  # extra string cleaning\n  dfm_remove(pattern= c(\"#*\",\"@*\", \"rt\",\"RT\",\"H/T\",\"h/t\",\".com\",\".ca\",\"amp\"))\n\n\nhead(twitter_dfm)\n\nDocument-feature matrix of: 6 documents, 16,469 features (99.96% sparse) and 0 docvars.\n       features\ndocs    🇨🇦 september marks huge milestone 190 years old come explore\n  text1  1         0     0    0         0   0     0   0    0       0\n  text2  0         1     1    1         1   1     1   1    1       1\n  text3  0         0     0    0         0   0     0   0    0       0\n  text4  0         0     0    0         0   0     0   0    0       0\n  text5  0         0     0    0         0   0     0   0    0       0\n  text6  0         0     0    0         0   0     0   0    0       0\n[ reached max_nfeat ... 16,459 more features ]\n\n\nsee top features\n\ntopfeatures( twitter_dfm )\n\ntoronto  reggae  canada     new    best     now    live       h    2022   today \n   1337     503     477     391     376     371     355     346     340     326 \n\n\nThe Twitter data used in this section has only the text of the tweets. If the full data file containing screen names, then the following would work, which allows for clustering users in a dendrogram.\n# --- group twitter users by screen name\ntwitter_dfm_groups = dfm_group(twitter_dfm, groups = screen_name)\n\n#--- select tweets that meet a criteria\n#     remove tweets with a frequency < 10 and tweets with >50 tokens in total\n\ntwitter_select = twitter_dfm_groups %>% \n    dfm_select( min_nchar = 2) %>% \n    dfm_trim( min_termfreq = 10)\n    \ntwitter_select = twitter_select[ ntoken( twitter_select ) > 50 , ]\n\n#-- clustering\n\ntwitter_dist = as.dist(  textstat_dist( twitter_select ))\ntwitter_clust = hclust( twitter_dist )\n\nplot( twitter_clust )\n\n2.6.1 multi-word expressions\nWe can compound multi-word expressions through collocation analysis.\n\ntext = c(\"Everyone loves New York!\", \"The home office of Barak Obama was in the White House.\", \"European Union is also known as the EU.\",\"Soon it will be Black Friday, crazy time.\")\n\ntext\n\n[1] \"Everyone loves New York!\"                              \n[2] \"The home office of Barak Obama was in the White House.\"\n[3] \"European Union is also known as the EU.\"               \n[4] \"Soon it will be Black Friday, crazy time.\"             \n\n\nnow tokenize the text and clean the text\n\ntext_corpus = corpus(text)\n\ntext_tokens = tokens(text_corpus, \n                     remove_punct = T, \n                     remove_separators = T) %>% \n  tokens_remove(stopwords(\"en\"))\n\ntext_tokens\n\nTokens consisting of 4 documents.\ntext1 :\n[1] \"Everyone\" \"loves\"    \"New\"      \"York\"    \n\ntext2 :\n[1] \"home\"   \"office\" \"Barak\"  \"Obama\"  \"White\"  \"House\" \n\ntext3 :\n[1] \"European\" \"Union\"    \"also\"     \"known\"    \"EU\"      \n\ntext4 :\n[1] \"Soon\"   \"Black\"  \"Friday\" \"crazy\"  \"time\"  \n\n\nOne of the most common type of multi-word expressions is proper names, which we can select simply based on capitalization in English texts.\n# this text example is too small to run but the code is valid\n\ntext_tokens2 = tokens_select(text_tokens,\n              pattern = \"^[A-Z]\",\n              valuetype = \"regex\",\n              case_insensitive = FALSE\n              )\n\ntext_collocat = textstat_collocations( text_tokens2, tolower = FALSE)\n\ntokens_compound(text_tokens, pattern = text_collocat[ text_collocat$z > 3])\nonce you have all your tokens compounded, you can search where they are in the corpus.\n\n# this is an imperfect example but shows the steps to use in large corpus\n\nkwic(text_tokens, pattern = \"Barak\")\n\nKeyword-in-context with 1 match.                                                   \n [text2, 3] home office | Barak | Obama White House\n\n\n\n\n2.6.2 related keywords\nWe can identify related words of keywords based on their distance in the documents.\nJust a reminder of steps for a corpus to be ready for the textstat_keyness()\n\ntokenize a corpus with tokens()\nclean tokens tokens_remove()\ntransform tokens into a dfm()\n\n\ninaug_keyness = textstat_keyness( inaug_tokens_dfm )\n\nhead(inaug_keyness)\n\n        feature     chi2            p n_target n_reference\n1     immutable 76.44488 0.000000e+00        2           1\n2   impressions 76.44488 0.000000e+00        2           1\n3  providential 76.44488 0.000000e+00        2           1\n4 deliberations 36.75264 1.341089e-09        2           4\n5    peculiarly 36.75264 1.341089e-09        2           4\n6     pecuniary 36.75264 1.341089e-09        2           4\n\n\n\n\n2.6.3 Naive Bayes classifier\nNaive Bayes is a supervised model usually used to classify documents into two or more categories. We train the classifier using class labels attached to documents, and predict the most likely class(es) of new unlabeled documents.\n\n# install.packages('caret')\nlibrary(caret)\n\nmovie_review = corpus(data_corpus_moviereviews)\n\n# head(movie_review)\n\nsummary(movie_review, 5)\n\nCorpus consisting of 2000 documents, showing 5 documents:\n\n            Text Types Tokens Sentences sentiment   id1   id2\n cv000_29416.txt   354    841         9       neg cv000 29416\n cv001_19502.txt   156    278         1       neg cv001 19502\n cv002_17424.txt   276    553         3       neg cv002 17424\n cv003_12683.txt   313    555         2       neg cv003 12683\n cv004_12641.txt   380    841         2       neg cv004 12641\n\n\nThe variable “Sentiment” indicates whether a movie review was classified as positive or negative. In this example we use 1500 reviews as the training set and build a Naive Bayes classifier based on this subset. In a second step, we predict the sentiment for the remaining reviews (our test set).\n\nset.seed(300)\n\nid_train = sample(1:2000, 1500, replace = FALSE)\n\nhead(id_train)\n\n[1]  590  874 1602  985 1692  789\n\n\n\n# create docvar with id\nmovie_review$id_num = 1:ndoc(movie_review)\n\n# tokenize texts\nmovie_review_tokens = tokens(movie_review,\n                             remove_punct = T,\n                             remove_numbers = T) %>% \n  tokens_remove(stopwords(\"en\")) %>% \n  tokens_wordstem()\n\n# movie review dfm\nmovie_review_dfm = dfm(movie_review_tokens)\n\n# TRAINING SET\nmovie_training = dfm_subset(movie_review_dfm, id_num %in% id_train)\n\n# TEST SET \nmovie_test = dfm_subset(movie_review_dfm, !id_num %in% id_train)\n\nNaive Bayes\n\nmovie_naiveBayes = textmodel_nb(movie_training, movie_training$sentiment)\n\nsummary(movie_naiveBayes)\n\n\nCall:\ntextmodel_nb.dfm(x = movie_training, y = movie_training$sentiment)\n\nClass Priors:\n(showing first 2 elements)\nneg pos \n0.5 0.5 \n\nEstimated Feature Scores:\n        plot      two      teen     coupl       go    church     parti\nneg 0.002579 0.002318 0.0002870 0.0007157 0.002663 8.719e-05 0.0002652\npos 0.001507 0.002338 0.0001656 0.0005456 0.002348 8.768e-05 0.0002728\n        drink     drive      get     accid      one       guy       die\nneg 1.199e-04 0.0003052 0.004486 9.445e-05 0.007389 0.0014458 0.0005485\npos 9.417e-05 0.0002630 0.003783 1.851e-04 0.007355 0.0009937 0.0005488\n    girlfriend   continu      see     life  nightmar      deal    watch\nneg  0.0003124 0.0003161 0.002557 0.001435 0.0001199 0.0004323 0.001642\npos  0.0002338 0.0003215 0.003020 0.002497 0.0001202 0.0005196 0.001539\n        movi     sorta     find   critiqu mind-fuck   generat     touch\nneg 0.010117 1.090e-05 0.001453 9.445e-05 3.633e-06 0.0002652 0.0002289\npos 0.007657 1.624e-05 0.001630 8.443e-05 3.247e-06 0.0002923 0.0004449\n         cool      idea\nneg 0.0003052 0.0008210\npos 0.0002273 0.0005845\n\n\nNaive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()\n\nmovie_dfm_match = dfm_match(movie_test, features = featnames(movie_training))\n\ninspect the classification model\n\nmovie_actual_class = movie_dfm_match$sentiment\n\nmovie_predicted_class = predict( movie_naiveBayes, newdata = movie_dfm_match )\n\nmovie_class_matrix = table( movie_actual_class, movie_predicted_class)\n\nmovie_class_matrix\n\n                  movie_predicted_class\nmovie_actual_class neg pos\n               neg 213  45\n               pos  37 205\n\n\nFrom the cross-table we see that the number of false positives and false negatives is similar. The classifier made mistakes in both directions, but does not seem to over- or underestimate one class.\n\ncaret::confusionMatrix(\n  movie_class_matrix,\n  mode= \"everything\",\n  positive= \"pos\"\n)\n\nConfusion Matrix and Statistics\n\n                  movie_predicted_class\nmovie_actual_class neg pos\n               neg 213  45\n               pos  37 205\n                                          \n               Accuracy : 0.836           \n                 95% CI : (0.8006, 0.8674)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.672           \n                                          \n Mcnemar's Test P-Value : 0.4395          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8520          \n         Pos Pred Value : 0.8471          \n         Neg Pred Value : 0.8256          \n              Precision : 0.8471          \n                 Recall : 0.8200          \n                     F1 : 0.8333          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4840          \n      Balanced Accuracy : 0.8360          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\n\nEnd of Quanteda section"
  },
  {
    "objectID": "Quanteda-corpus.html#naive-bayes-classifier",
    "href": "Quanteda-corpus.html#naive-bayes-classifier",
    "title": "2  Quanteda",
    "section": "2.7 Naive Bayes classifier",
    "text": "2.7 Naive Bayes classifier\nNaive Bayes is a supervised model usually used to classify documents into two or more categories. We train the classifier using class labels attached to documents, and predict the most likely class(es) of new unlabeled documents.\n\n# install.packages('caret')\nlibrary(caret)\n\nmovie_review = corpus(data_corpus_moviereviews)\n\n# head(movie_review)\n\nsummary(movie_review, 5)\n\nCorpus consisting of 2000 documents, showing 5 documents:\n\n            Text Types Tokens Sentences sentiment   id1   id2\n cv000_29416.txt   354    841         9       neg cv000 29416\n cv001_19502.txt   156    278         1       neg cv001 19502\n cv002_17424.txt   276    553         3       neg cv002 17424\n cv003_12683.txt   313    555         2       neg cv003 12683\n cv004_12641.txt   380    841         2       neg cv004 12641\n\n\nThe variable “Sentiment” indicates whether a movie review was classified as positive or negative. In this example we use 1500 reviews as the training set and build a Naive Bayes classifier based on this subset. In a second step, we predict the sentiment for the remaining reviews (our test set).\n\nset.seed(300)\n\nid_train = sample(1:2000, 1500, replace = FALSE)\n\nhead(id_train)\n\n[1]  590  874 1602  985 1692  789\n\n\n\n# create docvar with id\nmovie_review$id_num = 1:ndoc(movie_review)\n\n# tokenize texts\nmovie_review_tokens = tokens(movie_review,\n                             remove_punct = T,\n                             remove_numbers = T) %>% \n  tokens_remove(stopwords(\"en\")) %>% \n  tokens_wordstem()\n\n# movie review dfm\nmovie_review_dfm = dfm(movie_review_tokens)\n\n# TRAINING SET\nmovie_training = dfm_subset(movie_review_dfm, id_num %in% id_train)\n\n# TEST SET \nmovie_test = dfm_subset(movie_review_dfm, !id_num %in% id_train)\n\nNaive Bayes\n\nmovie_naiveBayes = textmodel_nb(movie_training, movie_training$sentiment)\n\nsummary(movie_naiveBayes)\n\n\nCall:\ntextmodel_nb.dfm(x = movie_training, y = movie_training$sentiment)\n\nClass Priors:\n(showing first 2 elements)\nneg pos \n0.5 0.5 \n\nEstimated Feature Scores:\n        plot      two      teen     coupl       go    church     parti\nneg 0.002579 0.002318 0.0002870 0.0007157 0.002663 8.719e-05 0.0002652\npos 0.001507 0.002338 0.0001656 0.0005456 0.002348 8.768e-05 0.0002728\n        drink     drive      get     accid      one       guy       die\nneg 1.199e-04 0.0003052 0.004486 9.445e-05 0.007389 0.0014458 0.0005485\npos 9.417e-05 0.0002630 0.003783 1.851e-04 0.007355 0.0009937 0.0005488\n    girlfriend   continu      see     life  nightmar      deal    watch\nneg  0.0003124 0.0003161 0.002557 0.001435 0.0001199 0.0004323 0.001642\npos  0.0002338 0.0003215 0.003020 0.002497 0.0001202 0.0005196 0.001539\n        movi     sorta     find   critiqu mind-fuck   generat     touch\nneg 0.010117 1.090e-05 0.001453 9.445e-05 3.633e-06 0.0002652 0.0002289\npos 0.007657 1.624e-05 0.001630 8.443e-05 3.247e-06 0.0002923 0.0004449\n         cool      idea\nneg 0.0003052 0.0008210\npos 0.0002273 0.0005845\n\n\nNaive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()\n\nmovie_dfm_match = dfm_match(movie_test, features = featnames(movie_training))\n\ninspect the classification model\n\nmovie_actual_class = movie_dfm_match$sentiment\n\nmovie_predicted_class = predict( movie_naiveBayes, newdata = movie_dfm_match )\n\nmovie_class_matrix = table( movie_actual_class, movie_predicted_class)\n\nmovie_class_matrix\n\n                  movie_predicted_class\nmovie_actual_class neg pos\n               neg 213  45\n               pos  37 205\n\n\nFrom the cross-table we see that the number of false positives and false negatives is similar. The classifier made mistakes in both directions, but does not seem to over- or underestimate one class.\n\ncaret::confusionMatrix(\n  movie_class_matrix,\n  mode= \"everything\",\n  positive= \"pos\"\n)\n\nConfusion Matrix and Statistics\n\n                  movie_predicted_class\nmovie_actual_class neg pos\n               neg 213  45\n               pos  37 205\n                                          \n               Accuracy : 0.836           \n                 95% CI : (0.8006, 0.8674)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.672           \n                                          \n Mcnemar's Test P-Value : 0.4395          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8520          \n         Pos Pred Value : 0.8471          \n         Neg Pred Value : 0.8256          \n              Precision : 0.8471          \n                 Recall : 0.8200          \n                     F1 : 0.8333          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4840          \n      Balanced Accuracy : 0.8360          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\n\nEnd of Quanteda section"
  },
  {
    "objectID": "TidyText.html",
    "href": "TidyText.html",
    "title": "3  TidyText",
    "section": "",
    "text": "This section is about the Text Mining with R book by Julia Silge and David Robinson, but delivered in a different way and with different examples. This section is not trying to copy nor reproduce their work, but with the goal of showing real world examples. This section will not do much explaining of concepts as the original material covers that.\nIf you read this book without reading the Text Mining with R book first, you will still hopefully be able to follow along and understand the functions."
  },
  {
    "objectID": "TidyText.html#libraries",
    "href": "TidyText.html#libraries",
    "title": "3  TidyText",
    "section": "3.1 libraries",
    "text": "3.1 libraries\nThe necessary libraries to follow along with this section.\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(readtext)   # read in various text file formats\nlibrary(rvest)      # web scrape \nlibrary(ralger)     # another web scraping library\nlibrary(wordcloud2) # customizable word cloud"
  },
  {
    "objectID": "TidyText.html#text",
    "href": "TidyText.html#text",
    "title": "3  TidyText",
    "section": "3.2 Text",
    "text": "3.2 Text\nYou need or want text to analyze, so let us scrape the web to get some! This is 2 skills in one, web scrape and tokenization of text. In this example we will web scrape the movie transcript of Star Trek II : The Wrath of Khan, as it is a beginner friendly example.\nIf you use Chrome browser and have SelectorGadget browser extension, this is very helpful in finding the HTML <tags> when scraping the web.\n\n# Star Trek transcripts url\n\ntrek_url = \"http://www.chakoteya.net/movies/movie2.html\"\n\n# use rvest to read the html\n# use SelectorGadget to find tag for all the text on page\n# retrieve only the text on page\n\ntrek_dialogue = rvest::read_html(trek_url) %>% \n  # this website has all text in the \"td\" tag\n  rvest::html_elements(\"td\") %>% \n  rvest::html_text2()\n\n\n3.2.1 Tokens\nThe trek_dialogue returns 1 very long string, this text needs to be tokenized in order for it to be used for analysis.\nStep one is to convert our long string into a data frame then tokenize it by using tidytext unnest_tokens() function.\n\ntrek_df = tibble( trek_dialogue) %>% \n  unnest_tokens(input = trek_dialogue, output = word)\n\nhead(trek_df)\n\n# A tibble: 6 × 1\n  word   \n  <chr>  \n1 opening\n2 credits\n3 in     \n4 the    \n5 23rd   \n6 century\n\n\n\n\n3.2.2 Clean tokens\nWe have a dataframe with tokenized words, but the words include stopwords which include words like: “a”, “but”, “such”, “sometimes”, etc. and have no value in text analysis. To remove all of these stopwords is by using tidytext stop_words.\n\ntrek_df_clean = trek_df %>% \n  anti_join(stop_words)\n\nJoining, by = \"word\"\n\nhead(trek_df_clean)\n\n# A tibble: 6 × 1\n  word     \n  <chr>    \n1 credits  \n2 23rd     \n3 century  \n4 bridge   \n5 simulator\n6 captain's\n\n\nWe went from 8,852 tokens to 4,070 tokens.\n\n\n3.2.3 Word count\nNow we have a cleaned tokenized data frame, we can find out what are the most common words in this movie Wrath of Khan.\n\ntrek_wordcount = trek_df_clean %>% \n  dplyr::count(word, sort = TRUE)\n\n# we can use ggplot to visualize\nlibrary(ggplot2) # just in case tidyverse did not include this\n\ntrek_wordcount %>% \n  # word counts greater than 50 \n  dplyr::filter(n > 50) %>% \n  ggplot(\n    aes( x= n, \n         y= reorder(word, n),\n         fill= n\n         )\n  ) +\n  geom_col() +\n  ggdark::dark_mode() +\n  labs(title = \"Word Count for Star Trek II: Wrath of Khan\",\n       y=\"\", \n       x=\"word count\", \n       fill=\"count\"\n       )\n\nInverted geom defaults of fill and color/colour.\nTo change them back, use invert_geom_defaults()."
  },
  {
    "objectID": "TidyText.html#sentiment",
    "href": "TidyText.html#sentiment",
    "title": "3  TidyText",
    "section": "3.3 Sentiment",
    "text": "3.3 Sentiment\nFor Sentiment Analysis you need to load in the sentiment lexicon libraries. The three main ones used are:\n\nAFINN\nbing\nnrc\n\nEach have their use-cases and licenses, but we will use bing here as it bins sentiments into ‘positive’ / ‘negative’.\nThe goal is to bind the words in our Star Trek data frame and the sentiment.\n\n# get the bing sentiment\nbing = get_sentiments(\"bing\")\n\n\ntrek_df_clean_sentiment = trek_df_clean %>% \n  count(word, sort = T) %>% \n  inner_join(bing) %>% \n  ungroup()\n\nJoining, by = \"word\"\n\n\n\nhead( trek_df_clean_sentiment)\n\n# A tibble: 6 × 3\n  word      n sentiment\n  <chr> <int> <chr>    \n1 death    12 negative \n2 dead     10 negative \n3 damn      9 negative \n4 kill      7 negative \n5 ready     7 positive \n6 warp      7 negative \n\n\n\ntrek_df_clean_sentiment %>% \n  filter(n > 3) %>% \n  ggplot(\n    aes(x= n, \n        y= sentiment,\n        fill= sentiment)\n  )+\n  geom_col()+\n  facet_wrap( ~ word, ncol = 2, scales = \"free_y\") +\n  ggdark::dark_mode()\n\n\n\n\nThe word warp is more of a neutral word but this Star Trek text is not common for everyday text analysis, so uncommon output is to be expected.\n\ntrek_df_clean_sentiment %>% \n  group_by(sentiment) %>% \n  slice_max(n, n= 5) %>% \n  ungroup() %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(\n    aes(x= n,\n        y= word,\n        fill= sentiment\n        )\n  )+\n  geom_col(show.legend = F) +\n  facet_wrap( ~sentiment, scales = \"free_y\")+\n  ggdark::dark_mode() +\n  labs(x= \"Contribution to sentiment\", y=\"\", title = \"Words that contribute to positive and negative sentiment: ST: Wrath of Khan\")"
  },
  {
    "objectID": "TidyText.html#wordclouds",
    "href": "TidyText.html#wordclouds",
    "title": "3  TidyText",
    "section": "3.4 Wordclouds",
    "text": "3.4 Wordclouds\nAs a person who tries to avoid white backgrounds of websites, plots, and documents I like using the library wordcloud2 as you can change the background color of the word cloud. With wordcloud2 is that you have interactive word clouds, when your mouse hovers over a word it tells you the number of times that word is counted (obviously when a word count is conducted).\n\nlibrary(wordcloud2)\n\n\ntrek_df_clean %>% \n  count(word, sort = T) %>% \n  filter(n > 10) %>% \n  wordcloud2(size = 1,\n             minSize = 0.8, \n             backgroundColor = \"black\", \n             color = \"random-light\",\n             shape = \"pentagon\"  #\"circle\"\n             )\n\n\n\n\n\n\n3.4.1 Sentiment wordcloud\nIf interested in specific sentiment words to make a word cloud based on word counts. Here is a positive word cloud based on Star Trek II: Wrath of Khan.\n\ntrek_df_clean_sentiment %>%\n  filter(sentiment == \"positive\") %>% \n  wordcloud2(\n    size = 0.8,\n    minSize = 0.6,\n    backgroundColor = \"black\",\n    color = \"random-light\"\n  )\n\n\n\n\n\nHere is the negative sentiment for our Star Trek text data frame.\n\ntrek_df_clean_sentiment %>%\n  filter(sentiment == \"negative\") %>% \n  wordcloud2(\n    size = 0.8,\n    minSize = 0.6,\n    backgroundColor = \"black\",\n    color = \"random-light\"\n  )"
  },
  {
    "objectID": "TidyText.html#document-term-frequency",
    "href": "TidyText.html#document-term-frequency",
    "title": "3  TidyText",
    "section": "3.5 document-term-frequency",
    "text": "3.5 document-term-frequency\nIn the Quanteda section this was called document-feature-matrix. The main query is how often in frequency does a word appear in a text, a chapter or book? To help answer that is to use the inverse term frequency and inverse document frequency (tdf). This tdfplaces less importance on words that are common and more importance on less common words. Together the inverse term frequency + inverse document frequency = tf_idf.\nNote that this section we do not remove stopwords as they will get a higher weight scale, where we care more about the high weight scale score for less common words.\nThis section will follow a similar example however we have different books and at this time the Gutenbergr package was giving errors on downloading more than one book. The goal is to look at word frequencies that differ chapters of an author’s book and between authors. The books selected here are:\n\nAlice’s Adventures in Wonderland, by Lewis Carroll\nThe Hunting of the Snark, by Lewis Carroll\nThe Time Machine, by H. G. Wells\nTales of Space and Time, by Herbert George Wells\n\nThe code below uses the url and downloads the text into an individual book, then each book is made into a dataframe with each book being labelled.\n\nbooks = tibble(\n  book = c(\"Alice's Adventures in Wonderland\", \n           \"The Hunting of the Snark\",\n           \"The Time Machine\",\n           \"Tales of Space and Time\"),\n  book_urls = c(\"https://www.gutenberg.org/files/11/11-0.txt\",\n                \"https://www.gutenberg.org/cache/epub/13/pg13.txt\",\n                \"https://www.gutenberg.org/files/35/35-0.txt\",\n                \"https://www.gutenberg.org/files/27365/27365-0.txt\")\n  ) %>% \n  dplyr::mutate(\n    book_text = purrr::map_chr(book_urls, \n                               ~.x %>% \n                                 rvest::read_html() %>% \n                                 rvest::html_text2()\n    ))\n\n\nhead(books)\n\n# A tibble: 4 × 3\n  book                             book_urls                             book_…¹\n  <chr>                            <chr>                                 <chr>  \n1 Alice's Adventures in Wonderland https://www.gutenberg.org/files/11/1… \"The P…\n2 The Hunting of the Snark         https://www.gutenberg.org/cache/epub… \"The P…\n3 The Time Machine                 https://www.gutenberg.org/files/35/3… \"The P…\n4 Tales of Space and Time          https://www.gutenberg.org/files/2736… \"Proje…\n# … with abbreviated variable name ¹​book_text\n\n\nNow that we have our text dataframe, we can tokenize the text.\n\nbook_tokens = books %>% \n  unnest_tokens(input = book_text, output = word)\n\n\nbook_words = book_tokens %>% \n  count(book, word, sort = T)\n\n\ntotal_book_words = book_words %>% \n  group_by(book) %>% \n  summarise( total = sum((n)))\n\ntotal_book_words\n\n# A tibble: 4 × 2\n  book                             total\n  <chr>                            <int>\n1 Alice's Adventures in Wonderland 29983\n2 Tales of Space and Time          76928\n3 The Hunting of the Snark          8433\n4 The Time Machine                 35960\n\n\n\nfull_book_words = left_join(book_words, total_book_words) %>% \n  mutate(percentage = n /total )\n\nJoining, by = \"book\"\n\nhead(full_book_words)\n\n# A tibble: 6 × 5\n  book                             word      n total percentage\n  <chr>                            <chr> <int> <int>      <dbl>\n1 Tales of Space and Time          the    5216 76928     0.0678\n2 Tales of Space and Time          and    3475 76928     0.0452\n3 Tales of Space and Time          of     2665 76928     0.0346\n4 The Time Machine                 the    2475 35960     0.0688\n5 Tales of Space and Time          a      1928 76928     0.0251\n6 Alice's Adventures in Wonderland the    1839 29983     0.0613\n\n\n\nggplot(\n  data = full_book_words,\n  aes(x = percentage , fill= book)\n)+\n  geom_histogram( show.legend = F, bins = 20)+\n  facet_wrap( ~book, ncol = 2, scales = \"free_y\")+\n  ggdark::dark_mode()+\n  labs(title = \"Term Frequency Distributions for 4 selected books\",\n       subtitle = \"stopwords are counted here\")\n\n\n\n\nThe plot shows that there are many non-stopwords used in these books.\n\n3.5.1 tf_idf function\nFind the words most important to a document or corpus and the weighted score value depends on how frequent that word is within the corpus. The tidytext has a function bind_tf_idf() to help find important words.\n\n0 = very common words\nnon-0 are words less common\n\n\nbooks_tf_idf = full_book_words %>% \n  bind_tf_idf(word, book, n)\n\nhead(books_tf_idf)\n\n# A tibble: 6 × 8\n  book                             word      n total perce…¹     tf   idf tf_idf\n  <chr>                            <chr> <int> <int>   <dbl>  <dbl> <dbl>  <dbl>\n1 Tales of Space and Time          the    5216 76928  0.0678 0.0678     0      0\n2 Tales of Space and Time          and    3475 76928  0.0452 0.0452     0      0\n3 Tales of Space and Time          of     2665 76928  0.0346 0.0346     0      0\n4 The Time Machine                 the    2475 35960  0.0688 0.0688     0      0\n5 Tales of Space and Time          a      1928 76928  0.0251 0.0251     0      0\n6 Alice's Adventures in Wonderland the    1839 29983  0.0613 0.0613     0      0\n# … with abbreviated variable name ¹​percentage\n\n\n\nbooks_tf_idf %>% \n  select( -total, -percentage ) %>% \n  arrange( desc(tf_idf))\n\n# A tibble: 18,640 × 6\n   book                             word        n      tf   idf  tf_idf\n   <chr>                            <chr>   <int>   <dbl> <dbl>   <dbl>\n 1 Alice's Adventures in Wonderland alice     386 0.0129  1.39  0.0178 \n 2 The Hunting of the Snark         snark      36 0.00427 1.39  0.00592\n 3 The Hunting of the Snark         bellman    32 0.00379 1.39  0.00526\n 4 Tales of Space and Time          lomi      202 0.00263 1.39  0.00364\n 5 Tales of Space and Time          denton    172 0.00224 1.39  0.00310\n 6 Alice's Adventures in Wonderland turtle     56 0.00187 1.39  0.00259\n 7 Alice's Adventures in Wonderland gryphon    55 0.00183 1.39  0.00254\n 8 Alice's Adventures in Wonderland hatter     55 0.00183 1.39  0.00254\n 9 The Hunting of the Snark         beaver     15 0.00178 1.39  0.00247\n10 Tales of Space and Time          mr        261 0.00339 0.693 0.00235\n# … with 18,630 more rows\n\n\n\nbooks_tf_idf %>% \n  select( -total, -percentage ) %>% \n  group_by( book ) %>% \n  slice_max(tf_idf, n= 15) %>% \n  ungroup() %>% \n  ggplot(\n    aes(x= tf_idf,\n        y= reorder(word, tf_idf),\n        fill= book\n        )\n  )+\n  geom_col(show.legend = F)+\n  facet_wrap(~book, ncol = 2, scales = \"free\")+\n  labs(x= \"tf_idf\", y=\"\", \n       title = \"Lewis Carroll vs H.G. Wells books\",\n       subtitle = \"term frequency-inverse document frequency\"\n       )+\n  ggdark::dark_mode()\n\n\n\n\nThe tf_idf plot show that H.G. Wells books on the right differ in words than Lewis Carroll books. Lewis Carroll uses more common words than H.G. Wells."
  },
  {
    "objectID": "TidyText.html#n-grams",
    "href": "TidyText.html#n-grams",
    "title": "3  TidyText",
    "section": "3.6 n-grams",
    "text": "3.6 n-grams\nWords in pairs instead of alone, single words without context. Use the unnest_tokens() function but pass in “ngram” the argument along with the number of words to include.\nThis example will use Twitter data, specifically tweets that had the hashtag “#Toronto” in the tweet, no other Twitter data is included here. The data was collected using the rtweet package. For this n-grams section it is important to use the rtweet library to read in the data as you will get errors when trying to make bigrams.\n\ntweets = rtweet::read_twitter_csv(\"./TorontoTweetsText.csv\")\n\nhead(tweets)\n\n# A tibble: 6 × 1\n  text                                                                          \n  <chr>                                                                         \n1 \"📌 @mlleperez\\n#Toronto #TheSix #EstudiaenCanadá🇨🇦 https://t.co/FzjSa3PhDv\"  \n2 \"September marks a huge milestone @DistilleryTO …190 years old! Come explore …\n3 \"The average home price rising again in Toronto.\\nhttps://t.co/NjHh7B5Odo\\n#t…\n4 \"Sault News: Real estate prices expected to remain stable | CTV News\\nhttps:/…\n5 \"Saturday vibes! #Toronto https://t.co/RKEosXP6X7\"                            \n6 \"#Toronto head over to #BloorWest to support #Ukraine️ this afternoon! https:/…\n\n\n\n3.6.1 bigrams\nSince we used the rweet library to read in the data, we are able to make bigrams. You will get an error if you use the read_csv() to load in the file.\n\n# unnest tokens \n# output is bigram\n# n= 2 \n# token = \"ngram\"  error if you use 'bigram'\ntweets_token = tweets %>% \n  unnest_tokens(input = text,\n                output = bigram,\n                n=2, \n                token = \"ngrams\"\n                )\n\n\n\n3.6.2 count n-grams\nLet us see what 2 words often appear in this twitter text data.\n\ntweets_token %>% \n  count( bigram, sort = T)\n\n# A tibble: 108,145 × 2\n   bigram             n\n   <chr>          <int>\n 1 https t.co      8657\n 2 toronto https   1131\n 3 in toronto      1055\n 4 toronto canada   551\n 5 to the           422\n 6 in the           342\n 7 the best         335\n 8 of the           332\n 9 from toronto     290\n10 the world        288\n# … with 108,135 more rows\n\n\nThe most common bigram is a short hyperlink “t.co”. Keep in mind that stopwords are still included in this text data.\nWe can make 2 columns each for the words that comprise of a bigram.\n\ntweet_bigrams = tweets_token %>% \n  separate( bigram, c(\"word1\",\"word2\"), sep = \" \")\n\n\ntwitter_bigram_filtered = tweet_bigrams %>% \n  # check to see if word1 & word2 are not in stopwords \n  filter( !word1 %in% stop_words$word) %>% \n  filter(!word2 %in% stop_words$word) %>% \n  # drop the website links\n  filter(!word1 %in% c(\"https\",\"t.co\")) %>% \n  filter(!word2 %in% c(\"https\",\"t.co\"))\n\nNow with filtered clean bigrams we can do a word count\n\ntwitter_bigram_counts = twitter_bigram_filtered %>% \n  count(word1, word2, sort = T)\n\nhead(twitter_bigram_counts)\n\n# A tibble: 6 × 3\n  word1   word2          n\n  <chr>   <chr>      <int>\n1 toronto canada       551\n2 toronto ontario      257\n3 air     nowplaying   246\n4 family  playing      246\n5 global  reggae       246\n6 reggae  family       246\n\n\nwhat is the words associated when word2 is “reggae” ?\n\ntwitter_bigram_filtered %>% \n  filter(word2 == \"reggae\") %>% \n  count(word1, sort = T)\n\n# A tibble: 30 × 2\n   word1              n\n   <chr>          <int>\n 1 global           246\n 2 rootsreggaehub   246\n 3 nights            10\n 4 canadian           5\n 5 julionking         3\n 6 bamboo             2\n 7 copa               2\n 8 grove              2\n 9 music              2\n10 rivoli             2\n# … with 20 more rows\n\n\nUnite the bigrams\n\ntwitter_bigrams_united = twitter_bigram_counts %>% \n  unite(bigram, word1, word2, sep = \" \")\n\ntwitter_bigrams_united\n\n# A tibble: 51,847 × 2\n   bigram                    n\n   <chr>                 <int>\n 1 toronto canada          551\n 2 toronto ontario         257\n 3 air nowplaying          246\n 4 family playing          246\n 5 global reggae           246\n 6 reggae family           246\n 7 reggae music            246\n 8 reggae reggaeallday     246\n 9 reggaeallday toronto    246\n10 rootsreggaehub reggae   246\n# … with 51,837 more rows\n\n\n\ntwitter_bigrams_united %>% \n  top_n(20) %>% \n  ggplot(\n    aes(x= n,\n        y= reorder(bigram, n),\n        fill= n\n        )\n  )+\n  geom_col(show.legend = F)+\n  ggdark::dark_mode()\n\nSelecting by n\n\n\n\n\n\n\n\n3.6.3 sentiment ngrams\nIt appears that the theme behind all the tweets were about reggae music. If word1 was reggae what would the second word be and how often would it appear?\n\ntweet_bigrams %>% \n  filter(word1 ==\"reggae\") %>% \n  count(word1, word2, sort = T)\n\n# A tibble: 19 × 3\n   word1  word2              n\n   <chr>  <chr>          <int>\n 1 reggae family           246\n 2 reggae music            246\n 3 reggae reggaeallday     246\n 4 reggae toronto           31\n 5 reggae artist             3\n 6 reggae reggaemusic        2\n 7 reggae world              2\n 8 reggae alive              1\n 9 reggae amp                1\n10 reggae and                1\n11 reggae bobmarley          1\n12 reggae feat               1\n13 reggae feelreggae         1\n14 reggae lane               1\n15 reggae mkrumah            1\n16 reggae pop                1\n17 reggae reggaelane         1\n18 reggae sat                1\n19 reggae upcomingartist     1\n\n\nload in the sentiment libraries from tidytext, this time we will use the AFINN library\n\n# sentiment\nAFINN = get_sentiments(\"afinn\")\n\ntoronto_words = tweet_bigrams %>% \n  filter(word1 ==\"toronto\") %>% \n  # add the sentiment library\n  inner_join(AFINN, by= c(word2 = \"word\")) %>% \n  count(word2, value,  sort = T)\n\n\nhead(toronto_words)\n\n# A tibble: 6 × 3\n  word2   value     n\n  <chr>   <dbl> <int>\n1 please      1     8\n2 welcome     2     7\n3 join        1     6\n4 crime      -3     4\n5 fire       -2     4\n6 great       3     4\n\n\nword1 being ‘toronto’, the most common word to follow it is ‘please’. Making “Toronto please” which has a positive sentiment value.\n\ntoronto_words %>% \n  mutate(contribution = n * value) %>% \n  arrange( desc( abs( contribution ))) %>% \n  head( 20 ) %>% \n  mutate( word2 = reorder(word2, contribution )) %>% \n  ggplot(\n    aes(x= contribution, \n        y= word2,\n        fill = contribution > 0)\n  ) +\n  geom_col(show.legend = F) +\n  ggdark::dark_mode() +\n  labs(title = \"Tweets with #Toronto, n = 7,985\",\n       subtitle = \"words split into bigrams\",\n       x= \"Sentiment value * number of occurrences (contribution)\",\n       y= \"Words preceded by \\\"Toronto\\\"\")\n\n\n\n\nto look at the negative sentiment word2 that follow ‘toronto’ arranged by most negative value.\n\ntoronto_words %>% \n  filter(value < 0) %>% \n  count( word2, value) %>% \n  arrange( value)\n\n# A tibble: 19 × 3\n   word2     value     n\n   <chr>     <dbl> <int>\n 1 fuck         -4     1\n 2 crime        -3     1\n 3 kill         -3     1\n 4 alone        -2     1\n 5 collision    -2     1\n 6 fire         -2     1\n 7 missing      -2     1\n 8 protest      -2     1\n 9 alert        -1     1\n10 attacks      -1     1\n11 bias         -1     1\n12 escape       -1     1\n13 eviction     -1     1\n14 hard         -1     1\n15 moody        -1     1\n16 no           -1     1\n17 pay          -1     1\n18 pressure     -1     1\n19 stop         -1     1"
  },
  {
    "objectID": "TidyText.html#word-graph",
    "href": "TidyText.html#word-graph",
    "title": "3  TidyText",
    "section": "3.7 word graph",
    "text": "3.7 word graph"
  }
]