
# Quanteda 

This section is for the Quanteda tutorials, which has 3 components (object types).

The 3 object types:

1. `corpus` = character strings and variables in data frames, combines texts with document level variables

2. `tokens` = tokens in a list of vectors, keeping the position of words

3. `document-feature-matrix` (DFM) = represents frequencies in a document in a matrix, no positions of words, can use bag-of-words analysis


## Libraries

Here is the list of Quanteda libraries to install and load in order to follow along with any part of the tutorial.

```{r, message=FALSE, warning=FALSE}
# install.packages("quanteda")
# install.packages("quanteda.textmodels") 
# devtools::install_github("quanteda/quanteda.corpora")
# devtools::install_github("kbenoit/quanteda.dictionaries")
# install.packages("readtext")

library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.corpora)
# used for reading in text data
library(readtext)
```

The `readtext` package supports: plain text files `(.txt), JSON, csv, .tab, .tsv, .xml, pdf, .doc, .docx, etc` and can be used to read a book for the [Project Gutenberg](https://www.gutenberg.org/) for text analysis.


After you have properly installed and loaded the libraries, you can access
the the data corpus available without the need of reading in a `csv` file.
Load in a corpus from Quanteda package and save it as a variable.

Corpus available:

- data_corpus_amicus
- data_corpus_dailnoconf1991
- data_corpus_EPcoaldebate
- data_corpus_immigrationnews
- data_corpus_inaugural  _most common used one in tutorial_
- data_corpus_irishbudget2010
- data_corpus_irishbudgets
- data_corpus_moviereviews
- data_corpus_sotu  _State of the Union speeches_
- data_corpus_udhr
- data_corpus_ukmanifestos
- data_corpus_ungd2017


```{r}
# to load a corpus and save it as variable name
corpus = corpus(data_char_ukimmig2010)
```



## Corpus

Load the corpus `data_corpus_ukmanifestos`, which is about British election manifestos on immigration and asylum.

```{r}
brit_manifesto = corpus(data_corpus_ukmanifestos,
                        docvars = data.frame(party = names(data_corpus_ukmanifestos)))


brit_manifesto
```

```{r}
head( summary(brit_manifesto) )
```




### docvars

Quanteda objects keep information associated with documents, these are 'document level variables'
and are accessed using `docvars()` function.

```{r}
inaug_corpus = corpus(data_corpus_inaugural)

head( docvars(inaug_corpus))
```



To extract `docvars` variables use the `field` argument or use the `$` like you normally use on a dataframe.


```{r}
docvars( inaug_corpus, field = 'Year')
```


Create or update `docvars`, in this example of creating a column for Century.

```{r}
floor_ = floor(docvars(inaug_corpus, field = 'Year') / 100)+1

docvars(inaug_corpus, field = 'Century') = floor_

head(docvars(inaug_corpus))
```



### subset

This section is about using the `corpus_subset()` function on all `docvars`.


```{r}
#--- get the Inaugural Speeches
# inaug_corpus

#--- look at the docvars
# head( docvars(inaug_corpus))

# now we subset the corpus for speeches after 1990
inaug_corpus_subset1990s = corpus_subset(inaug_corpus, Year >= 1990)

# check the number of documents 
head(inaug_corpus_subset1990s, 3)
```


If you want only specific US Presidents. _Note that there are 2 data objects that appear when you type 'President', `presidents` from the {datasets} package and `presidential` from {ggplot2} package_.


```{r}
# select specific US Presidents
selected_presidents = c('Obama','Clinton','Carter')

democrat_corpus_subset =  corpus_subset(inaug_corpus,
                                        President %in% selected_presidents
                                        )


democrat_corpus_subset
```





### reshape

You can reshape the document paragraphs to sentences, which can be restored even after being
modified by functions.


```{r}
UN_corpus = corpus(data_corpus_ungd2017)

# 196 documents, 7 docvars
head(UN_corpus)
```


Now change the document into sentences

```{r}
UN_corpus_sentences = corpus_reshape(UN_corpus, to= 'sentences')

head(UN_corpus_sentences)

# there is now 16806 number of documents
```

To change back 

```{r}
UN_corpus_doc = corpus_reshape(UN_corpus_sentences, to= 'documents')
```







### segment

You can extract segments of text and tags from documents, the use-case for this is
analyzing documents or transcripts separately.


Document sections


The created mini text document that has 6 lines, which has section that 
uses "##" as a pattern to organize the text into new lines. Here is what the
text file contains.

```{r}
##INTRO This is the introduction.
##DOC1 This is the first document.  Second sentence in Doc 1.
##DOC3    Third document starts here.  End of third document.
##INTRO                                               Document
##NUMBER                                      Two starts before
##NUMBER                                                 Three.
```



```{r}
library(readr)

# read in a simple text file as shown above
txt_file = readr::read_file('./doctext.txt')

# need to convert it to a corpus object
txt_file_corpus = corpus(txt_file)

# provide the pattern of ## in order for it to be separated into new lines
txt_seg = corpus_segment(txt_file_corpus, pattern = "##*")
txt_seg
```


By providing a pattern value for the `corpus_segment()` you can do regular expressions for
removing words and symbols.


```{r}
# transform text into corpus
corp_speeches = corpus("Mr. Samwell Tarly: Text Analysis.
                        Mx. Jones: More text analysis?
                        Mr. Samwell Tarly: we will do it again, and again.")

# use corpus segment with regular expression pattern matching
corp_speakers = corpus_segment(corp_speeches, 
                                pattern = "\\b[A-Z].+\\s[A-Z][a-z]+:", 
                                valuetype = "regex")

# bind the rows together
cbind(docvars(corp_speakers), text = as.character(corp_speakers))
```






































